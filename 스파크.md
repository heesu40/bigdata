

https://wikidocs.net/book/2350 scala참고!

## Scala

스칼라는 2004년 마틴 오더스키(Martin Odersky)가 발표한 **객체 지향 언어의 특징과 함수형 언어의 특징을 함께 가지는 다중 패러다임 프로그래밍 언어**다.

***

## 특징

1. 객체지향 언어의 특성 +함수형 언어의 특성(자바스크립트랑 파이썬, R도 비슷)
2. 자바가상머신(JVM)에서 동작하는 JVML언어
   - 자바가상머신위에서 동작하는 것은 scala,kotlin,groovy등
3. 자바 가산머신위에서 동작하기에 자바의 모든 라이브러리 사용 가능 
4. 스칼라 컴파일러를 통해 스칼라 코드로 변환하고, 바이트 코드는 JVM상에서 자바와 동일하게 실행(즉 플랫폼에 독립적이다, Windows나 Linux나 Mac에  동일하게 실행)

### 함수형 언어

1. 함수형 언어 특성으로 자바에 비해 코드 길이가 짧다.

   - **적은 양의 코드로 방대한 규모의 시스템을 작성할 수 있다.**
   - 스칼라에서는 모든것이 Object이기 때문에 ==로 모든 비교 가능

2. 객체지향 프로그래밍 언어와 함수형 프로그래밍 모두 완벽하게 지원

   - 모든 것이 객체이면 함수가 first object(함수적 프로그래밍 특성은 함수를 마치 하나의 값으로 취급하여 변수 또는 파라미터로 넘길 수 있다.)
   - 모든 것을 함수로 해결하면 의도하지 않는 동작이 발생 할 일이 없고, 한번 검증된 함수는 신뢰할 수 있기 때문에 버그가 줄어든다.
   - Immutable 변수는 문제를 단수화 시켜주어 데이터 공유, 병렬처리에 강하다.

   #### 함수 언어

   > 함수의 실행이 외부에 영향을 끼치지 않는 함수. 

   #### 익명 함수

   > 선언부가 없는 함수. 코드의 길이를 줄일 수 있다.
   >
   > Arrays.asList(1,2,3).stream().reduce((a,b) -> a-b).get();
   >
   > 여기서 reduce가 익명함수이다.

   #### 고차 함수

   > 함수를 인수로 취하는 함수. 함수를 입력 파라미터나 출력 값으로 처리 가능
   >
   > 

### 장점

1. 코드의 직관성, 신축성
2. 풍부한 표현식과 연산자
   - Fist-class function
   - Closure
3. 간결함
   - 타입 추론
   - 함수 리터럴(Literal)
4. 자바와의 혼용가능 객체지향+함수형 언어
   - 자바 라이브러리 재사용 가능
   - 자바 도구를 재사요 ㅇ가능
   - 성능의 손실 없이 사용 가능
   - 스칼라에서는 모든 것이 객체
5. 동시성에 강한 언어
   - 스칼라에서는 많은부분이 변경불가능 속성을 가지게끔 되어있다.
   - 아카(AKKa)라이브러리=동시성 프로그래밍에서 뛰어난 액터(=**함수**) 모델이용
   - 액터 모델은 각각의 액터가 서로 간의 메시지를 통해서만 의사소통을 하고  액터를 이루는 각종 변수나 속성은 서로 공유하지 않는다.
6. expression( 결과를 반환하는 문장) 중심 언어=표현중심언어
7. 필요할 때 implicit 예약어를 사용하면 명시적인 표현을 감출 수있다.

## Scala설치

1. 윈도우에서 해도 된다.
2. https://www.scala-lang.org/download/ (혹은 구글에서 스칼라 다운로드 검색) [scala-2.13.0.msi](https://downloads.lightbend.com/scala/2.13.0/scala-2.13.0.msi) 다운
3. Scala IDE-IntelliJ, or sbt,Scala's build tool. Http://scala-ide.org 
4. 이클립스는 마켓플레이스에서 스칼라 IDE를 설치하면 슼라라 프로젝트 생성 가능 scala ide 검색

#### 실행

1. cmd 창을 열고
2. scala 입력
3. prompt 가 열린다. (스칼라 셀은 스칼라 )
   - 안되는 경우 환경변수 추가 하자 
   - Path에서 `C:\Program Files (x86)\scala\bin` 추가

#### 기초 

```cmd
scala>1
scala>var a=1
scala>val b=1
scala>b=2 #하면 오류생긴다(val은 불변 변수 선언이므로)
scala>val c:Int=0 #변수 타입 지정
scala>1 to 10
sclaa>(1 to 10).toList
scala>1.toDouble #형변환
scala>1.0.toInt #형변환
```

- 스칼라의 기본 변수 타입은 모두 클래스
- 변수 선언 var,val(불변 변수 선언),변수 타입 생략 가능
- Range 타입 -1 to 10, 1 to 10 by 2 , 리스트나 배열 타입으로는 형변환 해야 한다.

```cmd
scala>:help
:edit
:help
:history
:load <path>
:quit
:save <path>
```

```scala
object Ex1{
def main(args: Array[String]): Unit={
println("Hello, Scala!")
}
}
```

메모장에 제목 "Ex1.scala" 로 저장

실행은 cmd 창에서 저장된 위치로 가서 

```cmd
C:\scala> scalac Ex1.scala # C폴더 아래 scala라는 파일을 생성한 상태, scalac 로 컴파일 
C:\scala> dir/w #파일 확인
C:\scala>scala Ex1
Hello, Scala!
#실행확인

```



- null보다는 none을 추천

- var는 언제든지 값이 바뀔 수 있는 일반적인 변수 선언

- val는 final변수 선언

- 변수 의 값으로 null이나 None으로 초기화 가능

- 컴파일러가 알아서 자료형에 대해 판단하고 필요하면 묵시적 형 변환을 통해 필요한 자료형으로 바꿔준다.

- 자료형을 명시적으로 선언하여 해당 자료형으로 값이 저장

- 기본 자료형은 자바에서 파생문자열인 String을 제외하고 AnyVal이라는 공통의 이름으로 불리며, 참조 자료형은 AnyRef로 불린다.

  ```cmd
  scala>var d:Int=0.1
  scala>var e=null
  scala>d=None
  scala>var f:Boolean=1 #에러가 남으로 호환이 안된다.
  
  scala>val str2="""a
  b
  c"""
  #멀티 라인 문자열은 세개의 쌍따옴표를 이용
  ```



#### 접두어

- 접두어를 이용한 처리

- 접두어 S

- ${변수명}을 이용하여 문자열 안의 변수를 값으로 치환

  ```cmd
  scala> val name="David"
  scala>println(s"Hello! ${name}") 
  sclala>println("${1+1}")
  scala>println(s"${1+1}")
  ```

- 접두어 F

- 문자열 포맷팅 처리 , 자바의 pirnf()와 같은 방식

  ```scala
  val height:Double=182.3
  val name="james"
  //f접두어를 이용한 값 변화 테스트
  println(f"$name%s is $height%2.2f meters tall")
  ```

- 접두어 raw

- 접수어 raw는 특수 문자를 처리하지 않고 원본 문자로 인식

  ```scala
  s"가\n나" //\n으로 개행 문자 처리(엔터)
  raw"가\n나"//\n을 문자 그대로 인식
  ```

##### 접두어 예제

```scala
var str3=s"println $str1"
println(str3)
println(s"2*3=${2*3}")
def minus(x: Int, y:Int)=x-y
println(s"${Math.pow(2,3)}") //2를 3번 곱한것 (지수승)
println(s"${minus(2,3)}")
```

#### Range타입

1. Range 타입 -1 to 10, 1 to 10 by 2, 리스트나 배열 타입으로는 형변환 해야한다

2. type 예약어는 자료형이 복잡한 경우 변칭을 주어 쓸 수 있게 한다.

   ```scala
   type Name=String
   type Person=(String,Int)
   type Ftype=String=>Int //함수 표현식
   val name: Name="홍길동"
   val person:Person=("korea",24)
   val f:Ftype=text=>text.toInt
   println(f) //객체 생성 됐따는 것
   f("34.125")//에러
   f(34.125)//에러 
   f(34) //에러
   f("34")//Int라 뜬다
   ```

#### 조건문, 반복문

- 조건문 : if /else
- 반복문 :for, while, do while

```scala
for(x<-1 to 10){
    //반복할 실행문
    println(x)// 10까지 나온다.
}
for(x<-1 range){
    //반복할 실행문
    println(x)
}
for(x<-1 until 10){
    //반복한 실행문
    println(x) //9까지 나온다.
}
//조건이 있는 반복문
for(i <-10) if (i % 2==0){
    println(i)
}
for(i <- 1 to 10) if (i % 2==0){
    println(i)
}
val lst=(10 to 100 by 10).toList
//인덱스가 있는 for ans
for((num,index) <- lst.zipWithIndex){
    println(s"index: $num")
}
```

- 이중 for 문

```scala
object Ex{
    def main(args: Array[String]): Unit={
        for(x <- 1 until 5; y <- 1 until 5){
            print(x + "*" + y + "=" + x*y +"|")
        }
    }
} //저장해서 scalac 하거나 for문만 cmd에서 scala에 실행
```



#### 함수

- 변수와 마찬가지로 :를 이용해 반환 자료형을 정의 , 반환 자료형이 함수의 자료형을 결정
- Unit은 자바의 void 자료형과 같다(반환 결과가 없는 함수에 붙는 자료형)
- 반환 값이 있을 때도 반환 자료형 생략 가능
- 명시적으로 return을 사용하는 경우, 함수 선언하는 곳에도 반환 자료형을 명시
- 반환 자료형을 명시한 경우 다른 자료형을 반환하려고 하면 에러 발생

```scala
def 함수([매개변수]): [반환 자료형]= {
//구현할 로직
}
```

```scala
object Ex{
def main(args: Array[String]): Unit = {
println("반환받은 값: "+ name())
}
def name()={
val a=10
a
}
}
```

```scala
def name() :  Int={
val a=10
return a
}
```

```scala
def name() = {
val a=10
return a
} //error :method name has return statement; needs result type
```

```scala
def name3(): Int={
val a=10
a
}
```

- 스칼라에서는 함수 인자의 타입을 명시해야한다

- 인자가 없는 함수의 경우 호출시 괄호 생략 가능

- ```scala
  def addOne(m: Int):Int =m+1
  val three =addOne(2)
  def three()=1+2
  three()
  three
  ```

- **이름없는 함수**를 만들 수 있다.

- 이름 없는 함수르 ㄹ다른 함수나 식에 넘기거나 val에 저장 가능

- 함수가 여러 식으로 이루어진 경우 {}를 사용해 이를 위한 공간 만들 수 있다.

- ```scala
  (x:Int) => x+1
  인터프리터가 부여한 이름(1)
  
  val addOne=(x:Int)=> x+1
  addOne(1)
  ```

- ```scala
  def timesTwo(i:Int):Int={
  println("hello world")
  i*2
  }
  timesTwo(2)
  
  {i:Int=>
  println("hello world")
  i*2} //즉시 실행 함수, 익명함수
  ```

- **(_) 사용하기**

- 함수 호출시 (_)사용하면 일부만 적용 가능, 그렇게 하면 새로운 함수 얻는다. 스칼라에서 밑줄은 문맥에 따라 의미가 다르다.

- 보통 이름없는 마법의 문자!

- 인자 중에서 원하는 어떤 것이든 부분 적용 가능, 위치는 아무곳이나 가능

- ```scala
  def adder(m: Int, n:Int) =m+n
  val add2=adder(2,_:Int)
  add2(3)
  ```

- **커리함수**

- 함수의 인자중 일부 적용하고, 나머지는 나중에 적용하게 남겨두는 것

- ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val thisYear = 2009
          val fixedValueFunction=go(thisYear, _:String)
          fixedValueFunction("test1")
          fixedValueFunction("test2")
          fixedValueFunction("test3")
      }
      def go(thisYear:Int, String:String)={
          println(String + ":" + thisYear)
      }
  }
  ```

- **=>**를 이용한 함수 표현식

- 스칼라 컴파일러는 = >표현식을 보면 function객체로 선언

- 스칼라는 매개변수가 하나면 function1, 두개면 function2,...22개 까지 function을 상속 하는 트레이트 제공

- ```scala
  def functionAsValue=(y:Int)=> y+10
  //컴파일러가 아래로 컴파일 한다
  val functionAsValue : Int=>Int=new Function(Int,Int){
      def apply(y:Int):Int=y+10
  }//컴파일러가 이리 해주기에 우리는 위의 한줄로만 써도 된다.
  ```

- **변수가 함수 넣기**

- 명시적으로 함수가 기대되지 않는 곳에서 =연산자를 이용해 매개변수가 필요한 함수를 대입하였을때 에러가 발생-객체화되어 있찌 않는 함수를 바로  val에 대입하면 에러 발생

- ```scala
  Object Ex{
      def main(args: Array[String]): Unit={
          val g=f_
          println(f(1))
      }
      def f(i:Int)=i
  }//에러발생?
  ```

- ```scala
  val g(Int=>Int)=f//이리 써야 한다?
  ```

- ```scala
  val g=f //는 그대로 두고
  //f 를 선언할 때 =>를 이용
  def f=(i:Int)=>i
  ```

- **재귀함수**

- ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val result=calc(x => x*x, 2,5)
          println(result)
      }
      def calc(f:Int = > Int, start: Int, end: Int) = {
          //합계를 구하는 재귀 함수
          def loop(index:Int, sum:Int):Int={
              if(index>end) sum//인덱스가 끝 값보다 크면 sum을 반환
              else loop(index+1, f(index)+sum)
          }
          loop(start,0)//루프를 최초 호출
      }
  }
  ```

- **매개변수가 여러 개인 함수**

  ```scala
  object Ex{
      def printlnStrings(args: String*)={
          for(arg <- args){
              println(arg);
          }
      }
  }
  
  printlnStrings("st1","st2","st3")
  printlnStrings( )
  
  val array1=Array("1","2","3","4")
  printlnStrings(array1) //String을 요구하는데 Array[String]가 들어가 오류 뜬다. 
  ```

-  **매개변수의 기본값 설정**

  ```scala
  object Ex{
      def default(a: Int=4,b:Int=5):Int=a+b
      println("기본값은"+default())
      println("변수값은"+default(11,6))
  }
  ```

- **apply**

- 매번 매서드 이름을 적는 것을 피하기 위해 사용

- 변수를 받아 함수에 적용시켜 결과를 만들어내는 설정자와 같은 역활

- Apply() 를 이용하면 생성자처럼 초기화하거나 클래스 안에 특정한 메서드를 기본 메서드로 지정하는 것을 편하게 할 수 있따.

  ```scala
  object Ex{
      class SomeClass{
          def apply(m:Int)=method(m)
          def method(i:Int)={
              println("method(Int) called")
              i+i
          }
          def method2(s:String)=5
      }
      val something=new SomeClass
      println(somethin(2))
  }
  ```

- **암묵적 형변환**

- Implicit 는 에러는 바로 내지 않고 해당하는 함수가 있으면 그것을 사용해서 암묵적으로 형 변환을 도와주어 함수의 활용도를 높힌다.

  ```scala
  object Sample{
  case class Person(name:String)
  //implicit def StringToPerson(name:String) :Person=Person(name) 이게 없으면 sayHello가 형변환이 안되고 이 게 있으면 자동 형변환된다
  def sayHello(p:Person): Unit ={
  print("Hello,"+p.name)
  }
  sayHello("korea")
  }
  ```

- 반환 자료형과 매개변수만으로 판단하기 때문에 에러 발생 할 수 있다.

  ```scala
  implicit def doubleToInt(d: Double)=d.toInt
  implicit def doubleToInt2(d: Double)=d.toInt+1
  val x:Int =18.0//에러 발생
  
  def doubleToInt(d:Double)=d.toInt
  val x:Int=doubleToInt(18.0)
  
  def doubleToInt(d:Double)=d.toInt
  val x:Int=18.0//에러 발생
  
  implicit def doubleToInt(d:Double)=d.toInt
  val x:Int =18.0
  ```

- **객체**

- 스칼라에서는 연산자와 메서드를 포함한 모든  것이 객체

- 객체 생성 방법

  - 클래스를 통한 인스턴스 화 -new 를 사용하여 계속 인스턴스 생성 가능
  - object예약어를 통해 객체 생성-싱글턴 객체로서 유일한 객체 생성

- 클래스

  - 클래스 안에서 메소드는 def로, 필드는 val로 정의한다. 메소드는 단지 클래스(객체)의 상태를 접근할 수 있는 함수에 지나지 않는다

- 스칼라에서는 public class대신 object예약어를 통해 처음부터 메모리에 객체 생성하고 컴파일러는 객체에 main이라는 이름이 있으면 main을 프로그램의 시작점으로 컴파일 한다.

  ```scala
  object Ex{
      def main(args: Array[String]): Unit={
          val apple=new Fruit("사과")
          println(apple.name)
      }
  }
  clss Fruit(input:String){
      var name=input
  }
  ```

- **케이스 클래스**

- 자동으로 멤버 변수를 만들어 주고, 외부에서도 멤버 변수에 접근이 가능하도록 한다. toString,hashCode,equals를 자동으로 만들어 준다.

  ```scala
  case class Fruit2(name:String)
  val apple=new Fruit2("사과")
  println(apple.name)
  val apple2=new Fruit2("사과")
  println(apple2.name)
  
  println(apple2.equals(apple)) //true
  println(apple.hashCode)
  println(apple.toString)//Fruit2(사과) 로 결과가 나온다.
  ```

- 스칼라에서는 생성자가 특별한 메소드를 따로 존재하지 않는다. 클래스 몸체에서 메소드 정의 부분밖에 있는 모든 코드가 생성자 코드가 된다.

  ```scala
  class Calculator(brand:String){
      //생성자
      val color:String=if(brand=="TI"){
          "blue"
      }else if(brand=="HP"){
      "black"
  }else{
      "white"
  }
  
  //인스턴스 메소드
  def add(m:Int, n:Int):Int=m+n
  }
  val calc=new Calculator("HP")
  calc.color
  ```

- 상속과 메소드 Overloading

  ```scala
  //상속
  class ScientificCalculator(brand:String) extends Calculator(brand){
      def log(m: Double,base:Double)math.log(m) / math.log(base)
  }
  
  ```

- **추상클래스 ** 는 메소드 정의는 있지만 구현은 없는 클래스이다. 대신 이를 상속한 하위클래스에서 메소드를 구현하게 된다. 추상 클래스의 인스턴스를 만들 수는 없다.

```scala
abstract class Shape{
    def getArea():Int
}
class Circle(r:Int)extends Shape{
    def getArea():Int ={r*r*3}
}
val s=new Shape //abstract이므로 변경 불가능
val c=new Circle(2) //
```

- **트레잇(trait)**
- 특성 : 하나의 완성된 기능이 아닌 어떤 한 객체에 추가될 수 있는 부가적인 특성
- 클래스의 부가적인 특성으로 동작, 자체로 인스턴스화는 가능하지 않다.
- 다른 클래스가 확장(즉, 상속) 하거나 섞어 넣을 수 있는(이를 믹스인 Mix in 이라 한다.)필드와 동작의 모음
- 클래스는 여러 트레잇을 with키워드를 사용해 확장
- 자유롭게 변수 선언하고 로직을 구현

```scala
trait Car{
    val brand: String
    
}
trait Shiny{
    val shineRefraction:Int
}
/*class BMW extends Car{
    val brand="BMW"
}*/
calss BMW extends Car with Shiny{//with로 확장
    val brand="BMW"
    val shineRefracton=12
}

//혹은
case class Car(brand:String)
case class Shiny(shineRefraction:Int)

```

- 추상클래스 대신 트레잇을 사용해야 하는 경우
- 인터페이스 역확을 하는 타입을  설계할때 트레잇과 추상클래스 두 가지 다 어떤 동작을 하는 타입을 만들수 있으며, 확장하는 쪽에서 일부를 구현하도록 요청
- 클래스는 오직 하나 만 상속 가능, 트레잇은 여러가지 상속 가능
- **트레잇 쌓기**
- 여러개를 한 클래스에서 상속받는데 상속에 문제가 생길경우(메소드를 호출했는데 다양한 클래스에 같은 메소드로 인한 충돌 발생) override 예약어와 함꼐 적당한 상속 관계를 만들 수 있다.
- 최종적인 상속받은 클래스의 메서드가 수행되도록 한다.

```scala
abstract class Robot{
    def shoot="뿅뿅"
}
trait M16 extends Robot{
    override def shoot="빵야"
}
trait Bazooka extends Robot{
    override def shoot="뿌왕뿌왕"
}
trait Daepodong extends Robot{
    override def shoot="콰르르르르를ㅇ"
}
class Mazinga extends Robot with M16 with Bazooka with Daepodong 
val robot=new Mazinga
println(robot.shoot)
//"콰르르르르르를ㅇ"이 출력된다.
```

- 모두의 기능을 실행하도록 상위클래스 super를 호출해서 해당하는 메서드를 실행되게 할 수 있다.

```scala
abstract class AnotherRobot{
    def shoot="뿅뿅"
}
trait M16 extends AnotherRobot{
    override def shoot=super.shoot + "빵야"
}
trait Bazooka extends AnotherRobot{
    override def shoot=super.shoot + "뿌왕뿌왕"
}
trait Daepodong extends AnotherRobot{
    override def shoot=super.shoot + "콰르르르르를ㅇ"
}
class Mazinga extends AnotherRobot with M16 with Bazooka with Daepodong 
val robot=new Mazinga
println(robot.shoot) //하면 모든것이 합쳐져서 나온다.
```

- **Compaion Object** static이 필요한 경우 대신 사용 가능
- **패턴 매치**
- 기본형이 아닌 튜플을 사용하는 경우 튜플 형식으로 변수를 정의해야 매칭된다.
- 리스트의 경우도 각 위치에 해당하는 값이 변수로 할당된다.
- 케이스 클래스의 경우는 클래스 형태를 그대로 사용해서 속성 정보를 매칭 할 수 있다.

```scala
val b=1
b match{
    case v if v==1 => println("b")
    case _ => println("err")
}
val c=(a,b)
c match{
    case(c1,c2) => println("c1:" + c1)
}
val d=List(1,3,5)
d match{
    case e1::e2::xs => 
}
```

```scala
def matchFunction(input:Any): Any =input match{
    case 100 => "hundred"
    case "hundred" => 100
    case etcNumber: Int => "입력값은 100이 아닌 Int형 정수입니다."
    case _ => "기타"
}
println(matchFunction(100))
println(matchFunction("hundred"))
println(matchFunction(1000))
println(matchFunction(1000.5))
println(matchFunction("thousand"))
```

```scala
case class Person(name:String, age:Int)
val alice=new Person("Alice",25)
val bob= new Person("Bob",32)
val charlie=new Person("Charlie",32)

for(person <- List(alice,bob,charlie)){
    person match{
        case Person("Alice",25) => println("Hi Alice!")
        case Person("Bob",32) => println("Hi Bob!")
        case Person(name,age)=> println(
        "Age: "+ age + " year, name: " + name + "?")
    }
}
```

- **Extractor로 패턴 매칭**
- Extractor는 패턴 매칭을 해야 하는 대상 값을 가져와서 필요한 값을 추출한 후 가공해서 내보낼 수 있다. 

```scala
object Emergency{
    def unapply(number: String):Boolean={
        if (number.length==3&&number.forall(_.isDigit)) true 
        else false
    }
}
object Normal{
    def unapply(number: String): Option[Int]={
        try{
            var o=number.replaceAll("-","")
            Some(number.replaceAll("-","").toInt)
        }catch{
            case _: Throwable => None
        }
    }
}
var number1="010-222-2222"
var number2="119"
var number3="포도 먹은 돼지"
var numberList=List(number1,number2,number3)
for(number <- numberList){
    number match{
        case Emergency() => println("긴급전화입니다.")
        case Normal(number) => println("일반전화입니다 - " + number)
        case _ => println("판단 할 수 없습니다.")
    }
}
```

- **컬렉션**
- ***배열*** 초기 값을 지정해서 배열을 선언하는 경우 자료형을 표시하지 않아도 알아서 자료형 할당

- ***리스트***  앞뒤가 연결된 리스트로서 내부적으로 리스트를 붙이거나 나누는데 효육적인 구조를 가지고 있다
- 동적으로 크기를 늘리거나 줄이는 것이 가능
- LIst는 추상 클래스 형태 혹은 이미 완성된 객체 형태로 존재하기 때문에 new를 사용하지 않는다
- 이미 만들어져있는 List정적 객체의 내부적인 팩토리 역활인 apply()가 동작하면서 새로운 List객체를 생성
- :: 는 리스트의 각 요소를 결합
- :::는 여러 리스트를 병합

```scala
val list1 ="a" :: "b" ::"c"::Nil
for(x <- 0 until list1.size)
println(list1(x))

val list2="d" ::"e"::Nil
val list0=list1 ::: list2
for(x <- 0 until list0.size){
println(list0(x))
}
```

- **Map**의 주요 기능
- 키를 통해 요소에 접근, 인덱스가 피룡하지 않다.

```scala
val list3="a"::"b"::"c"::Nil
val list4=1::2::3::Nil
val list5=2::2::4::Nil
println(list3 ++ list4)
println(list3.apply(0))
println(list3.reverse)
println(list4.max)
println(list4.min)
println(list4.sum)
println(list4.mkString(","))
println(list4.exists(a => 0>3))
println(list4.exists(_>3))
println(list4.contains(1))
println(list4.isEmpty)
println(list4.distinct)
```

```scala
val map=Map("number1" -> "aa",
            "number2" -> "bb",
           "number3" -> 3,
           5 -> "cc")
println(map.isEmpty) 
println("whole map:" +map)
println("keys:" +map.keys)
println("values: "+map.values)
println(map("number1"))
val map3= map + ("num4" -> 44)
println(map3)
val map2 =Map("n1"-> 100, "n2"-> 200)
val map4=map3 ++ map2
println(map4)
map4 - ("num4")
println(map4)
```

- **집합(set)**
- 중복되지 않는 값을 다뤄야 할때

```scala
var basket: Set[String] =Set()
basket+="딸기"
basket+="포도"
basket+="포도"
basket+="사과"
basket+="포도"
basket+="바나나"
println(basket)

var basket2: Set[String]=Set()
basket2 +="토마토"
basket2 +="당근"
basket2 +="감자"
basket2 +="사과"

println(basket.diff(basket2))
println(basket|baseket2)


```

- **튜플(tuple)**
- 여러 데이터를 하나의 묶음으로 처리하고 싶을 때 튜플로 처리
- 튜플은 N개의 데이터 쌍을 저장하는 자료 구조

```scala
val t1=(1,2) //튜플 생성
val t2=("a",1,"c")
val n1=t1._2//튜플 내용 참조 //2 결과값으로
val n2=t2._3// c 가 결과값으로
```

- **옵션**
- 여러 개의 값을 저장하는 자료 구조로서 값이 있을 수도 없을 수도 있는 경우에 사용
- None이거나 Some()을 하나 가지고 있다
- 맵에서 키를 이용해 값을 지겨울 떄, 해당하는 값이 있을 때는 Some()을 반환하고 없으면 None을 반환하여 로직에 사용
- 어떤 값이 들어 있으면 SOme 으로 래핑되어 있기 때문에  case Some()으로 패턴 매칭 할수 있고, 값이 없으면 None이기 떄문에 

```scala
object Ex{
    def main(args: Array[String]):Unit={
        val students=Map(
            1 -> "문진한",
            2 -> "엄다솔",
            3 -> "노순표"
           
        )
        val one=students.get(1)
        val four=students.get(4)
        
        println(one)
        println(four)
        println(one.get)
        println(four.getOrElse("값이 없습니다."))
    }
}
```

- **시퀀스**
- 내부적으로 인덱스에 대한 정보를 가지고 있고 인덱스와 관련해서 써야 할 기능이 많을 경우 쉽게 데이터를 다룰 수 있다.

```scala
val dounts: Seq[String] =Seq("Plain Dount", "Strawberry Dount","Glazed Dount")
println(s"Elements of dounts=$dounts")
println(s"Take elements from index 0 to 1=${dounts.slice(0,1)}")
println(s"Take elements from index 0 to 2=${dounts.slice(0,2)}")

```

- **이터레이터**
- hasNest와 next, length 등이 있다.

```scala
val list=List("a","b","c")
val i=list.iterator
while(i.hasNext)
println()
```

- **패턴매치**
- 스칼라에서는 패키지에 변수나 클래스 등을 선언 할 수 있다.
- 패키지 객체를 이용하면 Common과 같은 클래스를 정의하지 않고도 동일 패키지에서 사용하는 변수나 메서드 등을 공유 할 수 있다.
- package 키워드를 사용해 정의

```scala

```

- type은 새로운 타입을 선언하는 키워드
- 선언된 타입은 실제로 변수나 메서드의 타입으로 사용 가능
- 스칼라에서는 다른 클래스의 변수나 메서드 등을 사용하기 위해 import문을 사용
- 스칼라에서는 static키워드를 사용하지 않고, _를 사용해서 표기

- **함수 컴비네이터**
- 구현된 로직을 따라 컬렉션을 변형한 후 동일한 자료형의 컬렉션을 반환하는 역활을 맡는 메서드
- map(),foreach()컬렉션을 탐색하면서 그 안의 값을 바꾸는 역활
- map()리스트 자체를 변형하지 않고 List자료형을 반환 하면서 새로운 변수에 담게 한다
- foreach()아무값도 반환하지 않으며 리스트 자체의 값을 변형

```scala
val o=List(1,2,3,4)
println(o)
val n=o.map(i => i*10)
println(n)

val m=o.foreach(i => i*10)
```

- **filter(), filterNot()** -조건이 참, 거짓을 가릴 수 있는 형..?

```scala
val o =List(1,2,3,4)
println(o)
val n=o.filter(i => i => 3).map(i=>i*2)
println(n)
```

- **foldLeft(), foldRight()**컬렉션에 있는 여러 요소를 한쪽.

```scala
val o =List(1,2,3,4)
val sum=o.foldLeft(0.0)(_+_) 
println(s"Sum = $sum") //결과값은 10
```

- **partition()** 컬렉션을 나누는데 필요한 기능. 조건에 맞는 것들을 하나의 리스트로 저장하고 나머지 것을 다른 리스트에 저장

```scala
vol o=List(1,2,3,4)
val n=o.partition(i=> i<3)
println(n)
```

- **zip()** 두개의 리스트를 하나로 합친다.

```scala
val o=List(1,2,3,4)
val oo=List(5,6,7,8,9)
val n=o zip oo//<1,5), <2,6),(3,7),(4,8)
val nn=o:::oo//1,2,3,4,5,6,7,8,9
println(n)
println(nn)
```

- **find()** 원하는 조건에 맞는 첫 번째 요소를 반환

```scala
val o=List(1,2,3,4)
val n=o.find(i=> i>=2)
val nn=o.find(i=>i==9)
println(n)
println(nn)
```

- **drop(), dropWhile()**

```scala
val o=List(1,2,3,4,5,6,0)
val n=o.drop(4)//4 이하는 버린다.
val nn=o.dropWhile(i=> i<3)//조건식에 해당하는 것
println(n)
println(nn)
```

- **flattern()**리스트 안에 리스트가 중첩되어 있는 경우 풀어주는 기능을 수행

``` scala
val o=List(List(1,2,3,4),List(5,6))
val n=o.flatten
println(n)
```

```scala
val donuts1: Seq[String] =Seq("Plain","Strawberry","Glazed")
val donuts2: Seq[String]=Seq("Vanilla","Glazed")
val listDonuts: List[Seq[String]]=List(donuts1,donuts2)
val listDonutsFromFlatten:List[String]=listDonuts.flatten
```

- **예외 처리**
- try, catch, finally

```scala
import java.io.FileReader
import java.io.FileNotFoundException
import java.io.IOException
import java.io.Console

object Demo{
    def main(args: Array[String]): Unit={
        try{
            val f=new FileReader("input.txt")
            //읽는 것이 없기에 파일이 이써도 확인 못한다
            
        }catch{
            case ex: FileNotFoundException => {
                println("Missing file exception")
            }
            case ex: IOException => {
                println("IO Exception")
            }
        }finally{
            println("Exiting finally")
        }
    }
}
```

```scala
//파일 내용 읽기위해서
object Demo{
    def main(args: Array[String]){
        print("Please enter your input :")
        val line=Console.readLine
        println("Thanks, yout just typed: "+line)
    }
}
import scala.io.Source
object Demo{
    def main(args: Array[String]):Unit ={
        println("Following is the content read:")
        Source.fromFile("Demo.txt").foreach{
            print
        }
    }
}
```

```scala
//파일 내용 출력 하기 위해서
import scala.io.StdIn.readLine
import java.io.File
import java.io.PrintWriter


object Ex{
    def main(args: Array[String]):Unit ={
        val fileName="test.txt"
        var input=readLine("파일에 쓸 내용 입력 하세요!")
        
        val writer=new PrintWriter(new File(fileName))
        writer.write(input)
        writer.close
        //출력파일은 패키지 디렉토리에 생성된다.
        print( "입력하신 텍스트를 " + fileName+"에 저장했습니다.")
    }
}
```

- **either**
- 둘 중 하나를 선택

```scala
object Ex {   
   def main(args: Array[String]): Unit = {
       val input = scala.io.StdIn.readLine("값을 입력하세요:")
       val result: Either[String, Int] = try {
          Right(input.toInt)
       } catch {
         case e : Exception => Left(input)
       }
       print(result.getClass)             
   }
}

```





# 스파크

## 1. 정의

오픈소스 병렬분산 처리 플랫폼으로 스트림처리와 머신러닝을 효과적으로 수행하기 위함이다.

#### 1.1 스파크 구조

##### 1.1.1 복수의 컴포넌트로 구성 

1. sql처리
2. 스트림처리
3. 머신러닝
4. 그래프 처리(그래스X)

##### 1.1.2구성 요소

1. 클러스터 매니저 - Spark standalone, Yarn, Mesos
2. 코어 엔진- 스파크 코어
3. 라이브러리(스파크 sql(SQL처리), 스파크 스트리밍(스트림처리), MLlib(머신러닝) ,그래프X(그래프처리))

#####  1.1.3 스파크 자료구조

1. Spark에서의 데이터 처리하기 위한 추상화된 모델: RDD(복구가능한 분산 데이터셋)

2. SparkApplication => job

3. Spark 클러스터 환경에서의 node들 : SparkClient, Master 노드, Worker노드

4. SparkClient 역활

   > SparkApplication 배포하고 실행 요청

5. Spark Master 역활

   > Spark클러스터 환경에서 사용가능한 리소스들 관리

6. WorkNode 역활

   > 할당받은 리소스(CPU core,memory)를 사용해서 SparkApplication 실행
   >
   > Spark Worknode에서 실행되는 프로세스 -Executor는 RDD의 partition을 

##### 1.1.4 구현

SparkApplication 구현 단계

> 1. SparkContext 생성
>
>    SparkContext 은 스파크 진입점, 실행환경 클라이언트
>
> 2. RDD 생성
>
>    collection, HDFS, hive, CSV 등 으로 생성 가능 
>
>    불변데이터 모델, partition가능
>
> 3. RDD 처리
>
>    변환연산(RDD의 요소의 구조 변경, filter처리, grouping,...) RDD를 인수로 받아 새로운 RDD를 생성. **불변, 지연**특성을 가짐
>
> 4. 집계요약처리
>
>    Action연산(가공하지 않는 활동)
>
> 5. 영속화
>
>    파일로 저장 할 수 있다.

##### 1.1.5 장점

1. 메모리 기반임으로 반복처리와 연속으로 이루어지는 변환처리를 고속화 가능
2. 딥러닝,머신러닝들의 실행환경에 적합한 환경 제공
3. 서로 다른 실행환경과 구조, 데이터들의 처리에 대해서 통합 환경 제공

##### 1.1.6 메서드

- **sc.textFile()** :file로 부터 RDD생성
- **collect **배열만들기
- **Map , flatMap()**
- **mkString("구분자")**   RDD요소 출력.  배열의 요소를 문자열과 결합

***

## 설치하기

1. https://www.apache.org/dyn/closer.lua/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz 에서

   http://apache.tt.co.kr/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz 클릭해서 다운

```cmd
su -
cd usr/local
tar -zxvf /home/hadoop/Downloads/spark-2.4.3-bin-hadoop2.7.tgz
ls -l

ln -s spark-2.4.3-bin-hadoofp2.7 spark #심볼릭링크 만들기
ls -l
chown -R hadoop:hadoop spark
ls -l
```

 2. 

```cmd
su hadoop
vi .bash_profile
#설정 저장 위치는 
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
# export PATH=$PATH:$JAVA_HOME 에 추가해야한다
/bin:$SPARK_HOME/          # 애는 잠시 지운다. bin:$SBT_HOME/bin
PATH=$PATH:$HOME/.local/bin:$HOME/bin

export JAVA_HOME=/usr/local/jdk1.8.0_221
export HADOOP_HOME=/usr/local/hadoop-2.7.7
export HADOOP_CMD=/usr/local/hadoop-2.7.7/bin/hadoop
export HADOOP_STREAMING=/usr/local/hadoop-2.7.7/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar
export HIVE_HOME=/usr/local/hive
export SPARK_HOME=/usr/local/spark
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:
#path가 밑에잇어야 한다. 그러므로 path 위에 export를 추가하자
source .bash_profile
```



#### 클러스터 구성의 스파크 동작 확인

리눅스의 하둡계정을 로그아웃했다가 로그온 해주자~

```cmd
#hadoop 계정에서
spark-shell --master local verbose
scala> #로 들어가진다.
```



#### 실행해보자

```scala
//로컬 파일시스템에서 파일을 읽어들여서 RDD로 생성
scala> val file=sc.textFile("file:///usr/local/spark/README.md")
//RDD로부터 한 행(라인)단위로 처리-단어 분리 후 RDD 저장 생성
scala> val words=file.flatMap(_.split(" "))
//여기까지가 변환처리다

//같은 단어끼리 모아서 요약(개수) 계산-map형태로 단어와 출현횟수
scala> val result=words.countByValue

scala> result.get("For").get

```



## sbt 설치하기

스파크 애플리케이션은 소스코드를 컴파일 하고 JAR파일로 패키징해야 하는데 이를 해주는 것이 sbt이다. sbt는 스칼라와 자바로 기술된 소스코드를 통합 관리하기 위한 툴이다.(패키지 작성, 애플리케이션 개발 프로젝트 빌드 프로세스 관리 등 )

https://www.scala-sbt.org/download.html  sbt 1.2.7.tgz를 받자

```cmd
#root 계정(su -)
[root@master ~]# tar -zxvf /home/hadoop/Downloads/sbt-1.2.7.tgz -C /opt/
[root@master ~]# ls -l /opt
# 소유자가 hadoop이라 안 바꿔도 된다.


[hadoop@master ~]$ vi .bash_profile

export SBT_HOME=/opt/sbt
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:$SBT_HOME/bin
[hadoop@master ~]$ source .bash_profile
#로그아웃 했다가 로그온!

[hadoop@master ~]$ sbt about
#로 정보 확인
```



#### sbt로 스파크 애플리케이션 작성

1. sbt 프로젝트 디렉토리 구조 만들기

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir  spark-simple-app

#소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ cd spark-simple-app/
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala

#sbt 설정 파일 저장 디렉토리
[hadoop@master spark-simple-app]$ mkdir project

#소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example/
# 뒤의 새게의 파일이 바로 package명이다!
[hadoop@master example]$ vi SundayCount.scala

```

#### SundayCount.scala 작성

```javascript
package lab.spark.example

import org.joda.time.{DateTime,DateTimeConstants}
import org.joda.time.format.DateTimeFormat
import org.apache.spark.{SparkConf,SparkContext}

object SundayCount{
def main(args: Array[String]){
if (args.length<1){
throw new IllegalArgumentException(
"명령 인구 경로")
}

val filePath=args(0)
val conf=new SparkConf
val sc=new SparkContext(conf)

try{
val textRDD=sc.textFile(filePath)
val dateTimeRDD=textRDD.map{dateStr=>
val pattern=
DateTimeFormat.forPattern("yyyyMMdd")
DateTime.parse(dateStr,pattern)ㅗㅝㅓ
}
val sundayRDD=dateTimeRDD.filter{dateTime=>
dateTime.getDayOfWeek==DateTimeConstants.SUNDAY
}
val numOfSunday=sundayRDD.count
println(s"주어진 데이터에는 일요일이${numOfSunday}개 들어 있습니다.")
}finally{
sc.stop()
}
}
}

```

#### build.sbt 작성



```cmd
[hadoop@master ~]$ cd ~/spark-simple-app
[hadoop@master spark-simple-app]$ vi build.sbt

name :="spark-simple-app"
version :="0.1"
scalaVersion:="2.11.12"
libraryDependencies ++=
Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided",
"joda-time" % "joda-time" %"2.8.2")
assemblyOption in assembly :=(assemblyOption in assembly).value.copy(includeScala=false)

```



#### plugins.sbt작성과 sbt-assembly이용에 필요한 설정

```cmd
[hadoop@master spark-simple-app]$ cd project
[hadoop@master project]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n"%"sbt-assembly"%"0.14.10")
# 저장하고 나오자~
[hadoop@master project]$ cd ~/spark-simple-app/
[hadoop@master spark-simple-app]$ sbt assembly

```

Packaging /home/hadoop/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  여기에 jar가 생겼다.

jar로 패키지 한 것!

#### jar파일 실행해보자. 

1. 먼저 date.txt를 만들고 하둡에 올린다!

```cmd
[hadoop@master spark-simple-app]$ cd ~
[hadoop@master ~]$ vi date.txt

20150322
20150331
20150417
20150426
20150506
20150523
20150524
20150712
20150727
20150810
20150830
20150927

# 작성 (공백없이 작성해달라)


[hadoop@master ~]$ hadoop fs -mkdir /data/spark/
[hadoop@master ~]$ hadoop fs -put date.txt /data/spark
[hadoop@master ~]$ hadoop fs -ls /data/spark
Found 1 items
-rw-r--r--   1 hadoop supergroup        116 2019-08-27 13:37 /data/spark/date.txt
# 확인하자


[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.SundayCount --name SundayCount ~/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/spark/date.txt

#"" 오류가 생겼다면 date.txt의 공백을 다 지운상태에서 하둡의 파일을 지우고 다시 -put 하자~ 그리고 다시 assembly
```



### 스파크 WordCount 예제

1. SparkContext생성
2. (입력 소스로부터) RDD 생성
3. RDD 처리
4. 결과 파일 처리
5. SparkContext종료

```scala

```



## 스파크 셀

1. Spark Context
   - 애플리케이션과 스파크 클러스터와의 연결을 담당
   - 모든 스파크 애플리케이션은 SparkContext를 이용하여 RDD나 accumulator또는 broadcase변수 등을 다룬다.
   - 스파크 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는다.
2. RDD 생성
3. 스파크 클러스터
4. 분산 데이터로서 RDD
5. 파티션

### SparkContext

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-app

[hadoop@master ~]$ cd wordcount-app

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCount.scala
#---------------------------------------------------------------------------------------------
package lab.spark.example

import org.apache.spark.{SparkConf,SparkContext}

object WordCount{
def main(args: Array[String]){
require (args.length>=1,
"드라이버 프로그램의 인수에 단어를 세고자 하는"+
"파일의 경로를 지정해 주세요.")

val conf=new SparkConf
val sc =new SparkContext(conf)

try{
val filePath=args(0)
val wordAndCountRDD = sc.textFile(filePath)
.flatMap(_.split("[ ,.]"))
.filter(_.matches("""\p{Alnum}+"""))
.map((_, 1))
.reduceByKey(_+_)

wordAndCountRDD.collect.foreach(println)
}finally{
sc.stop()
}
}
}

#---------------------------------------------------------------------------------------------
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ sbt assembly

#데이터 소스 생성
[hadoop@master ~]$ vi simple-words.txt
cat
dog
.org
cat
rabbit
bear
cat
&&
tiger
dog
rabbit
100
bear
tiger
cat
rabbit
?bear

#하둡 파일 시스템에 simple-words.txt파일 업로드
[hadoop@master ~]$ hadoop fs -mkdir  /data/wordcount
[hadoop@master ~]$ hadoop fs -put simple-words.txt  /data/wordcount


[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.WordCount --name WordCount ~/wordcount-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/wordcount


spark-submit --master local --class lab.spark.example.wc --name wc ~/wc/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/exercise4

```

#### 스파크 RDD 

- **cogroup**
- RDD의 구성요소가 키와 값의 쌍으로 이루어진 경우 사용 가능

```scala
var rdd=sc.parallelize(List(("k1","v1"),("k2","v2"),("k1","v3")))
var rdd2=sc.parallelize(List("k1","v4"))
var result=rdd.cogroup(rdd2)
result.collect.foreach{
    case(k, (v_1,v_2))=> {
        println(s"($k,[${v_1.mkString(",")}],[$(v_2.mkString(", "))])")
    }
}
```



- **distinct()**
- RDD원소에서 중복을 제외한 요소로만 구성

```scala
var rdd=sc.parallelize(List(1,2,3,1,2,3,1,2,3))
var result=rdd.
```



- **substract()**
- 차집합

```scala
val rdd1 = sc.parallelize( List("a", "b", "c", "d", "e"))
val rdd2 = sc.parallelize( List("d", "e"))
val result = rdd1.substract(rdd2)
println(result.collect.mkString(", "))

```

- **intersecton()**
- 두개의 RDD가 있을 때 rdd1과 rdd2에 동시에 속하는 요소로 구성된 새로운 RDD를 생성하는 메서드
- 결과로 생성되는 RDD에는 중복된 원소가 존재하지 않는다.

```scala
val rdd1 = sc.parallelize( List( "a", "a", "b", "c"))
val rdd2 = sc.parallelize( List( "a", "a", "c", "c"))
val result = rdd1.intersection(rdd2)
println(result.collect.mkString(", "))

```

- **leftOuterJoin(), rightOuterJoin()**
- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용 가능
- 조인 결과를 표시할 떄는 값이 존재하지 않는 경우를 고려해 Option타입을 이용

```scala
val rdd1 = sc.parallelize( List( "a", "a", "b", "c")).map((_, 1))
val rdd2 = sc.parallelize( List( "b", "c")).map((_, 2))
val result1 = rdd1.leftOuterJoin(rdd2)
val result2 = rdd1.rightOuterJoin(rdd2)
println("Left:" + result1.collect.mkString("\t"))
println("Right:" + result2.collect.mkString("\t"))

```

- **substractByKey()**
- rdd1의 요소 중 rdd2에 같은 키가 존재하는 요소를 제외한 나머지로 구성된 새로운 RDD

```scala
val rdd1 = sc.parallelize( List("a", "b")).map(_, 1))
val rdd2 = sc.parallelize( List("b" )).map((_, 2))
val result = rdd1. substractByKey(rdd2)
println(result.collect.mkString("\n"))

```

- **reduceByKey()**
- RDD의 구성요소가 키와 값으로 구성된 경우에 사용 가능
- 같은 키를 가진 값들을 하나로 병합하여 키-값 쌍으로 구성된 새로운 RDD생성

```scala
var rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
var result = rdd.reduceByKey(rdd)
println(result.collect.mkString(","))

```

- **foldByKey()**

```scala
val rdd = sc.parallelize( List( "a", "b", "b")).map((_, 1))
val result = rdd.foldByKey(_+_)
println(result.collect.mkString(","))

```

- **combineBykey()**
- 값을 병합하기 위해 사용
- 파티션 단위로 사용 가능

```scala
def  reduceByKey(func: (V, V) => V) : RDD[(K, V)]
def  foldByKey(zeroValue: V)(func: (V, V) => V) : RDD[(K, V)] 
combineByKey[C](createCombiner:(V)=>C, mergeValue:(C, V)=>C, mergeCombiners: (C, C) => C):RDD[(K, C)]

```



```scala
//콤바이너 역활을 할 Record클래스 정의
case class Record(var amount: Long, var number: Long=1){//메서드 파라미터에서 기본값 설정
    def map(v:Long)=Record(v)
    def add(amount:Long):Record={
        add(map(amount))
    }
    def add(other:Record):Record={
        this.number+=other.number
        this.amount += other.amount
        this
    }
    override def toString =s"avg:${amount/number}"
}
//combineByKey()를 이용 한 평균값 계산
val data =Seq(("Math",100L),("Eng",80L),("Math",50L),("Eng",60L),("Eng",90L))
val rdd=sc.parallelize(data)
val createCombiner=(v:Long)=>Record(v)
val mergeValue =(c:Record,v:Long)=>c.add(v)
val mergeCombiners=(c1:Record,c2:Record)=>c1.add(c2)
val result=rdd.combineByKey(createCombiner,mergeValue,mergeCombiners)
println(result.collect.mkString("\n"))

(Math,avg:75)
(Eng,avg:76)


```

#### 집계와 관련된 연산

- **aggregateByKey()**

- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우 사용 가능

- 병합 시작할 초기값 생성, combineByKey()와 동일한 동작 수행

- **pipe()**

- Pipe를 이용하면 데이터를 처리 도중 외부 프로세스 활용 가능

- **repartitionAndSortWithinPartitions()**

- ```scala
  import org.apache.spark.HashPartitioner
  
  val r = scala.util.Random
  val data = for (i <- 1 to 10) yield (r.nextInt(100), "-")//난수 생성
  val rdd1 = sc.parallelize(data)
  val rdd2 = rdd1.repartitionAndSortWithinPartitions(new HashPartitioner(3))//hash방식은 파티션을 균등하게 가지는 것, 3개로 분할이 될 거다
  //결과 검증
  rdd2.foreachPartition(it => { println("========"); it.foreach(v=>println(v)) })//파티션 단위로 반복문을 통해 출력
  
  ```



#### 데이터 순서 바꾸어 처리하기

```scala
scala>val textRDD=sc.textFile("/data/spark/README.md")
//"file:///usr/local/spark/README.md" 여기에 있는 파일을 하둡의 위에 위치에 저장해놓자.

scala>val wordCandidateRDD=textRDD.flatMap(_.split("[ ,.]"))
scala>val wordRDD=wordCandidateRDD.filter(_.matches("""\p{Alnum}+"""))
scala>val wordAndOnePairRDD=wordRDD.map((_, 1))
scala>val wordAndCountRDD=wordAndOnePairRDD.reduceByKey(_+_)

scala> val countAndWordRDD=wordAndCountRDD.map{wordAndCount => (wordAndCount._2, wordAndCount._1)}

scala> val sortedCWRDD=countAndWordRDD.sortByKey(false)
scala> val sortedWCRDD=sortedCWRDD.map{countAndWord=> (countAndWord._2,countAndWord._1)}
scala> val sortedWCArray=sortedWCRDD.collect
scala> sortedWCArray.foreach(println)

```



#### RDD 선두로부터 요소 꺼내기

```
// top 3만 뽑기
val top3WordArray=sortedWCRDD.take(3)
top3WordArray.foreach(println)
```

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-top3

[hadoop@master ~]$ cd wordcount-top3

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCountTop3.scala
```

```java
package lab.spark.example

import org.apache.spark.{SparkConf, SparkContext}

object WordCountTop3{
    def main(args: Array[String]){
        require(args.length>=1,
               "드라이버 프로그램의 인수에 단어를 세고자 하는 "+
               "파일의 경로를 지정해 주세요.")
            val conf=new SparkConf
            val sc=new SparkContext(conf)
            try{
                //모든 단어에 대해(단어, 등장횟수)형의 튜플을 만든다.
                val filePath=args(0)
                val wordAndCountRDD =sc.textFile(filePath)
                    .flatMap(_.split("[ ,.]"))
                    .filter(_.matches("""\p{Alnum}+"""))
                    .map((_,1))
                    .reduceByKey(_+_)
                    //등장 횟수가 가장 많은 단어 세 개를 찾는다.
                    val top3Words=wordAndCountRDD.map{
                    case(word,count)=>(count,word)}.sortByKey(false).map{
                    case(count,word)=>(word,count)}.take(3)
                    //등장 획수가 가장 많은 단어 top3를 표준 출력으로 표시
                    top3Words.foreach(println)
                }finally{
                sc.stop()
            }
                }
            }
 

```

```cmd
[hadoop@master ~]$ cd ~/wordcount-top3
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-top3
[hadoop@master ~]$ sbt assembly
[hadoop@master ~]$ cd ~
[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.WordCountTop3 --name WordCountTop3 ~/wordcount-top3/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/wordcount

(rabbit,3)
(cat,3)
(tiger,2)
 
#로 결과값 나온다.

```

#### 복수의 데이터를 결합해 처리

```csv
products.csv==============================
0,송편(6개),12000
1,가래떡(3개),16000
2,연양갱,5000
3,호박엿(6개),16000
4,전병(20장),4000
5,별사탕,3200
6,백설기,3500
7,약과(5개),8300
8,강정(10개),15000
9,시루떡,6500
10,무지개떡,4300
11,깨강정(5개),14000
12,수정과(6컵),19000
13,절편(10개),15000
14,팥떡(8개),20000
15,생과자(10개),17000
16,식혜(2캔),21000
17,약식,4000
18,수수팥떡(6개),28000
19,팥죽(4개),16000
20,인절미(4개),10000
 
  

sales-october.csv===============================

5830,2014-10-02 10:20:38,16,28
5831,2014-10-02 15:13:04,15,22
5832,2014-10-02 15:21:53,2,10
5833,2014-10-02 16:22:05,18,13
5834,2014-10-06 12:04:28,19,18
5835,2014-10-06 12:54:13,10,18
5836,2014-10-06 15:43:54,1,8
5837,2014-10-06 17:33:19,10,22
5838,2014-10-11 10:28:00,20,19
5839,2014-10-11 15:00:32,15,3
5840,2014-10-11 15:06:04,15,14
5841,2014-10-11 15:45:38,18,1
5842,2014-10-11 16:12:56,4,5
5843,2014-10-13 10:13:53,3,12
5844,2014-10-13 15:02:23,15,19
5845,2014-10-13 15:12:08,6,6
5846,2014-10-13 17:17:20,10,9
5847,2014-10-18 11:08:11,15,22
5848,2014-10-18 12:01:47,3,8
5849,2014-10-18 14:25:25,6,10
5850,2014-10-18 15:18:50,10,16
5851,2014-10-20 13:06:00,11,21
5852,2014-10-20 16:07:04,13,29
5853,2014-10-20 17:29:24,5,4
5854,2014-10-20 17:47:39,8,17
5855,2014-10-23 10:02:10,2,24
5836,2014-10-23 11:22:53,8,19
5857,2014-10-23 12:29:16,7,7
5858,2014-10-23 14:01:56,12,26
5859,2014-10-23 16:09:39,8,13
5860,2014-10-23 17:26:46,8,19

sales-november.csv====================================
5861,2014-11-01 10:47:52,15,22
5863,2014-11-01 11:44:54,8,26
5864,2014-11-01 14:29:51,18,10
5865,2014-11-01 17:50:00,6,17
5867,2014-11-04 10:03:57,15,16
5868,2014-11-04 11:22:55,15,13
5869,2014-11-04 16:32:09,19,6
5870,2014-11-10 11:12:30,17,27
5871,2014-11-10 13:32:53,17,13
5872,2014-11-10 15:31:21,4,15
5873,2014-11-10 16:03:01,6,5
5874,2014-11-10 17:52:20,12,28
5875,2014-11-15 11:36:39,3,5
5876,2014-11-15 14:08:26,9,7
5877,2014-11-15 15:05:21,10,0
5878,2014-11-18 11:17:09,7,16
5879,2014-11-18 14:50:37,9,3
5880,2014-11-18 16:23:39,4,20
5881,2014-11-18 17:28:31,18,25
5882,2014-11-22 10:50:24,7,26
5883,2014-11-22 11:43:31,3,3
5884,2014-11-22 12:57:22,4,12
5885,2014-11-22 15:20:17,19,25
5886,2014-11-25 16:42:07,10,27
5887,2014-11-25 17:38:03,14,0
5888,2014-11-25 18:30:36,10,8
5889,2014-11-25 18:41:57,11,10
5890,2014-11-30 14:30:08,11,17
5862,2014-11-30 14:57:47,8,22
5866,2014-11-30 15:17:29,8,24


```

```cmd
[hadoop@master ~]$ vi products.csv
[hadoop@master ~]$ vi sales-october.csv
[hadoop@master ~]$ vi sales-november.csv
#각각 저장 후 put 해주자.
[hadoop@master ~]$ hadoop fs -mkdir /data/csv
[hadoop@master ~]$ hadoop fs -put /home/hadoop/sales-november.csv  /data/csv
[hadoop@master ~]$ hadoop fs -put /home/hadoop/sales-october.csv  /data/csv 
 [hadoop@master ~]$ hadoop fs -put /home/hadoop/products.csv  /data/csv

```

```scala
def createSalesRDD(csvFile:String)={
    val logRDD =sc.textFile(csvFile)
    logRDD.map{
        record=>
        val splitRecord=record.split(",")
        val productId=splitRecord(2)
        val numOfSold=splitRecord(3).toInt
        (productId,numOfSold)
    }
}

val salesOctRDD=createSalesRDD("/data/csv/sales-october.csv")
val salesNovRDD=createSalesRDD("/data/csv/sales-november.csv")

import org.apache.spark.rdd.RDD
def createOver50SoldRDD(rdd:RDD[(String,Int)])={
    rdd.reduceByKey(_+_).filter(_._2 >= 50)
}
val octOver50SoldRDD=createOver50SoldRDD(salesOctRDD)
val novOver50SoldRDD=createOver50SoldRDD(salesNovRDD)

val bothOver50SoldRDD=octOver50SoldRDD.join(novOver50SoldRDD)
bothOver50SoldRDD.collect.foreach(println)

val over50SoldAndAmountRDD=bothOver50SoldRDD.map{
    case(productId,(octAmount,novAmount))=>
    (productId,octAmount+novAmount)
}
over50SoldAndAmountRDD.collect.foreach(println)
(8,(68,72))
(15,(80,51))

```

#### 브로드캐스트 변수

```scala
import scala.collection.mutable.HashMap
import java.io.{BufferedReader,InputStreamReader}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem,Path}
val productsMap =new HashMap[String,(String,Int)]
val hadoopConf=new Configuration
val fileSystem=FileSystem.get(hadoopConf)
val inputStream=fileSystem.open(new Path("/data/csv/products.csv"))
val productsCSVReader=new BufferedReader(new InputStreamReader(inputStream))
var line=productsCSVReader.readLine
while(line!=null){
    val splitLine=line.split(",")
    val productId=splitLine(0)
    val productName=splitLine(1)
    val unitPrice=splitLine(2).toInt
    productsMap(productId)=(productName,unitPrice)
    line = productsCSVReader.readLine
}
productsCSVReader.close()

val broadcasetedMap=sc.broadcast(productsMap)

val resultRDD=over50SoldAndAmountRDD.map{
    case(productId,amount)=>
    val productsMap=broadcasetedMap.value
    val(productName,unitPrice)=productsMap(productId)
    (productName,amount,amount * unitPrice)
}
resultRDD.collect.foreach(println)

(강정(10개),140,2100000)
(생과자(10개),131,2227000)

```



spark-submit --master local --class lab.spark.example.QuestionnaireSummarization --name Questionnaire ~/Questionnaire/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/scv/

#### questionnaireRDD  집계처리

```cmd
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir 

[hadoop@master ~]$ cd Questionnaire

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi QuestionnaireSummarization.scala
#-----------------------------------------
```



```java
package lab.spark.example

import org.apache.spark.{Accumulator, SparkConf, SparkContext}
import org.apache.spark.rdd.RDD

object QuestionnaireSummarization {

  /**
   * 모든 앙케이트의 평가 평균값을 계산하는 메소드
   */ 
  private def computeAllAvg(rdd: RDD[(Int, String, Int)]) = {
    val (totalPoint, count) =
      rdd.map(record => (record._3, 1)).reduce {
        case ((intermedPoint, intermedCount), (point, one)) =>
          (intermedPoint + point, intermedCount + one)
      }
    totalPoint / count.toDouble
  }

  /**
   * 연령대별 평강의 평균값을 계산하는 메소드
   */
  private def computeAgeRangeAvg(rdd: RDD[(Int, String, Int)]) = {
    rdd.map(record => (record._1, (record._3, 1))).reduceByKey {
      case ((intermedPoint, intermedCount), (point, one)) =>
        (intermedPoint + point, intermedCount + one)
    }.map {
      case (ageRange, (totalPoint, count)) =>
        (ageRange, totalPoint / count.toDouble)
    }.collect
  }

  /**
   * 남녀별 평가의 평균값을 계산하는 메소드
   */
  private def computeMorFAvg(
      rdd: RDD[(Int, String, Int)],
      numMAcc: Accumulator[Int],
      totalPointMAcc: Accumulator[Int],
      numFAcc: Accumulator[Int],
      totalPointFAcc: Accumulator[Int]) = {
    rdd.foreach {
      case (_, maleOrFemale, point) =>
        maleOrFemale match {
          case "M" =>
            numMAcc += 1
            totalPointMAcc += point
          case "F" =>
            numFAcc += 1
            totalPointFAcc += point
        }
    }
    Seq(("Male", totalPointMAcc.value / numMAcc.value.toDouble),
      ("Female", totalPointFAcc.value / numFAcc.value.toDouble))
  }

  def main(args: Array[String]) {
    require(
      args.length >= 2,
       """
        |애플리케이션의 인자에
        |<앙케이트 CSV 파일의 경로>
        |<출력하는 결과파일의 경로>를 지정해 주세요. """.stripMargin)

    val conf = new SparkConf
    val sc = new SparkContext(conf)

    try {
      val filePath = args(0)

      // 앙케이트를 로드해 (연령대, 성별, 평가) 형식의
      // 튜플을 요소로 하는 RDD를 생성한다.
      val questionnaireRDD = sc.textFile(filePath).map { record =>
        val splitRecord = record.split(",")
        val ageRange = splitRecord(0).toInt / 10 * 10
        val maleOrFemale = splitRecord(1)
        val point = splitRecord(2).toInt
        (ageRange, maleOrFemale, point)
      }

      // questionnaireRDD는 각각의 집계처리에 이용되기 때문에 캐시에 보존한다
      questionnaireRDD.cache

      // 모든 평가의 평균치를 계산한다
      val avgAll = computeAllAvg(questionnaireRDD)
      // 연령대별 평균치를 계산한다
      val avgAgeRange = computeAgeRangeAvg(questionnaireRDD)

      // 성별이 M인 앙케이트의 건수를 세는 어큐뮬레이터
      val numMAcc = sc.accumulator(0, "Number of M")
      // 성별이 M인 앙케이트의 평가를 합계하는 어큐뮬레이터
      val totalPointMAcc = sc.accumulator(0, "Total Point of M")
      // 성별이 F인 앙케이트의 건수를 세는 어큐뮬레이터
      val numFAcc = sc.accumulator(0, "Number of F")
      // 성별이 F인 앙케이트의 평가를 합계하는 어큐뮬레이터
      val totalPointFAcc = sc.accumulator(0, "TotalPoint of F")

      // 남여별 평균치를 계산한다
      val avgMorF = computeMorFAvg(
        questionnaireRDD,
        numMAcc,
        totalPointMAcc,
        numFAcc,
        totalPointFAcc)

      println(s"AVG ALL: $avgAll")
      avgAgeRange.foreach {
        case (ageRange, avg) =>
          println(s"AVG Age Range($ageRange): $avg")
      }

      avgMorF.foreach {
        case (mOrF, avg) =>
          println(s"AVG $mOrF: $avg")
      }
    } finally {
      sc.stop()
    }
  }
}
```

```cmd


[hadoop@master ~]$ cd ~/Questionnaire
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")
# csv 파일 저장
[hadoop@master ~]$ vi questionnaire.csv
23,F,3
22,F,5
20,M,4
35,F,2
33,F,4
18,M,4
28,M,5
42,M,3
18,M,3
56,F,2
53,M,1
30,F,4
19,F,5
17,F,4
33,M,4
26,F,3
22,F,2
27,M,4
45,F,2
[hadoop@master ~]$ hadoop fs -put /home/hadoop/questionnaire.csv  /data/csv  
# 하둡에 저장


#어플리케이션 빌드
[hadoop@master ~]$ cd ~/Questionnaire
[hadoop@master ~]$ sbt assembly
[hadoop@master ~]$ cd ~
[hadoop@master ~]$ spark-submit --master local --class lab.spark.example.QuestionnaireSummarization --name Questionnaire ~/Questionnaire/target/scala-2.11/spark-simple-app-assembly-0.1.jar /data/csv/questionnaire.csv/ /oooooo
# 코드안에 경로에 저장하는 코드가 없기 때문에 저장은 안된다~
```



## 스파크 SQL



```scala
//RDD로부터 DataFrame생성
case class Dessert(menuId:String, name:String, price:Int,kcal:Int)

val dessertRDD=sc.textFile("/data/DataFrame/dessert-menu.csv")
val dessertDF=dessertRDD.map{record=>
     val splitRecord=record.split(",")
     val menuId=splitRecord(0)
      val name =splitRecord(1)
    val price=splitRecord(2).toInt
   val kcal=splitRecord(3).toInt
     Dessert(menuId,name,price,kcal)}.toDF

dessertDF.printSchema

//DataFrame으로부터 RDD생성
var rowRDD=dessertDF.rdd
val nameAndPriceRDD=rowRDD.map{row=>
     val name=row.getString(1)
     val price=row.getInt(2)
     (name,price)}

scala> nameAndPriceRDD.collect.foreach(println)
(초콜릿 파르페,4900)
(푸딩 파르페,5300)
(딸기 파르페,5200)
(판나코타,4200)
(치즈 무스,5800)
(아포가토,3000)
(티라미스,6000)
(녹차 파르페,4500)
(바닐라 젤라또,3600)
(카라멜 팬케익,3900)
(크림 안미츠,5000)
(고구마 파르페,6500)
(녹차 빙수,3800)
(초코 크레이프,3700)
(바나나 크레이프,3300)
(커스터드 푸딩,2000)
(초코 토르테,3300)
(치즈 수플레,2200)
(호박 타르트,3400)
(캬라멜 롤,3700)
(치즈 케익,4000)
(애플 파이,4400)
(몽블랑,4700)

//DataFrame에 쿼리 발행
scala> dessertDF.registerTempTable("dessert_table")
scala> val numOver300KcalDF=spark.sqlContext.sql("SELECT count(*) AS num_of_over_300Kcal From dessert_table WHERE kcal >= 300")
scala> numOver300KcalDF.show
+-------------------+
|num_of_over_300Kcal|
+-------------------+
|                  9|
+-------------------+
//스파크 SQL 내장 함수의 사용 예
scala> spark.sqlContext.sql("SELECT atan2(1, 3)" ).show
+-------------------------------------------+
|ATAN2(CAST(1 AS DOUBLE), CAST(3 AS DOUBLE))|
+-------------------------------------------+
|                         0.3217505543966422|
+-------------------------------------------+
//하이브 내장 함수의 사용 예
scala> spark.sqlContext.sql("SELECT pi() AS PI, e() AS E").show
+-----------------+-----------------+
|               PI|                E|
+-----------------+-----------------+
|3.141592653589793|2.718281828459045|
+-----------------+-----------------+




```



- 스파크 SQL의 내장 함수

  https://gool.gl/ukXtJH

- 하이브의 내장 함수

  https://goo.gl/Vda7h

```scala
val nameAndPriceDF=dessertDF.select(dessertDF("name"),dessertDF("price"))
nameAndPriceDF.printSchema

root
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)



val selectAllDF=dessertDF.select("*")
selectAllDF.printSchema
root
 |-- menuId: string (nullable = true)
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)
 |-- kcal: integer (nullable = false)


nameAndPriceDF.show

+---------------+-----+
|           name|price|
+---------------+-----+
|  초콜릿 파르페| 4900|
|    푸딩 파르페| 5300|
|    딸기 파르페| 5200|
|       판나코타| 4200|
|      치즈 무스| 5800|
|       아포가토| 3000|
|       티라미스| 6000|
|    녹차 파르페| 4500|
|  바닐라 젤라또| 3600|
|  카라멜 팬케익| 3900|
|    크림 안미츠| 5000|
|  고구마 파르페| 6500|
|      녹차 빙수| 3800|
|  초코 크레이프| 3700|
|바나나 크레이프| 3300|
|  커스터드 푸딩| 2000|
|    초코 토르테| 3300|
|    치즈 수플레| 2200|
|    호박 타르트| 3400|
|      캬라멜 롤| 3700|
+---------------+-----+
only showing top 20 rows


selectAllDF.show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-0|초콜릿 파르페| 4900| 420|
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
+------+-------------+-----+----+
only showing top 3 rows

```

- 1000원의 가격을 달러로 표시

```scala
val nameAndDollarDF=nameAndPriceDF.select($"name",$"price"/lit(1000.0))
nameAndDollarDF.printSchema

val nameAndDollarDF=nameAndPriceDF.select(
$"name",($"price"/lit(1000.0)) as "dollar price")

nameAndDollarDF.printSchema

//필터링(where())
val over5200WonDF=dessertDF.where($"price" >=5200)
over5200WonDF.show

val over5200WonNameDF=dessertDF.where($"price" >=5200).select($"name")
over5200WonNameDF.show
//정렬(orderBy())
val sortedDessertDF=dessertDF.orderBy($"price".asc,$"kcal".desc)
sortedDessertDF.show
//집약처리(agg())
val avgKcalDF=dessertDF.agg(avg($"kcal") as "avg_of_kcal")
avgKcalDF.show

import org.apache.spark.sql.types.DataTypes._
val numPerPriceRangeDF=dessertDF.groupBy(
(($"price"/1000) cast IntegerType)*1000 as "price_range").agg(count($"price")).orderBy($"price_range")
numPerPriceRangeDF.show
```

- DataFrame끼리의 결합

```cmd
SID-0,D-0,2
SID-0,D-3,1
SID-1,D-10,4
SID-2,D-5,1
SID-2,D-8,1
SID-2,D-20,1
[hadoop@master ~]$ vi dessert-order.csv
[hadoop@master ~]$ hadoop fs -put /home/hadoop/dessert-order.csv /data/DataFrame

```

```scala
case class DessertOrder(sId: String, menuId: String, num: Int)
val dessertOrderRDD=sc.textFile("/data/DataFrame/dessert-order.csv")
val dessertOrderDF=dessertOrderRDD.map{record=>
val splitRecord=record.split(",")
val sId=splitRecord(0)
val menuId=splitRecord(1)
val num=splitRecord(2).toInt
    DessertOrder(sId,menuId,num)
}.toDF

val amntPerMenuPerSlipDF=
dessertDF.join(
dessertOrderDF,
dessertDF("menuId")=== dessertOrderDF("menuId"),"inner").select($"sId",$"name",($"num"*$"price") as "amount_per_menu_per_slip")

amntPerMenuPerSlipDF.show

val anmtPerSlipDF=amntPerMenuSlipDF.groupBy($"sId").agg(sum($"amount_per_menu_per_slip") as "amount_per_slip").select($"sId",$"amount_per_slip")
anmtPerSlipDF.show

//스파크의 SQL의 UDF이용하기
val strlen=sqlContext.udf.register("strlen",(str:String)=> str.length)
sqlContext.sql("SELECT strlen('Hello 스파크 SQL') AS result_of_strlen").show
```



## Spark SQL

spark -shell 실행

```cmd
[hadoop@master hadoop-2.7.7]$ spark-shell --master local
```

```scala
//########DataFrameWriter와 DataFrameReader로 구조화 데이터 read/write 실습################
case class Dessert(menuId: String, name: String, price: Int, kcal: Int)
val dessertRDD = sc.textFile("/data/dessert-menu.csv") // 이 파일 루트는 csv가 있는 하둡위치로 바꾸자.
val dessertDF = dessertRDD.map{ record => 
    val splitRecord = record.split(",")
    val menuId = splitRecord(0)
    val name = splitRecord(1)
    val price = splitRecord(2).toInt
    val kcal = splitRecord(3).toInt
    Dessert(menuId, name, price, kcal)
    }.toDF
//앞의 내용이 선행되어야 한다.
val dfWriter=dessertDF.write
dfWriter.format("parquet").save("/data/dessert/dessert_parquet")
//이리되면 dessert_parquet디렉토리과 생김과 동시에 Parquet포맷 파일 생성된다.

val dfReader=spark.sqlContext.read
val dessertDF2=dfReader.format("parquet").load("/data/dessert/dessert_parquet")
dessertDF2.orderBy($"name").show(3) //이름순서이기 때문에 ㄱ ㄴㄷ ...순서로 간다.
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|  D-11|고구마 파르페| 6500| 650|
|  D-12|    녹차 빙수| 3800| 320|
|   D-7|  녹차 파르페| 4500| 380|
+------+-------------+-----+----+
only showing top 3 rows


```



#### 테이블 형식의 구조화된 데이터셋 다루기(테이블 형식으로 저장, 조회)

```scala
dessertDF.write.format("parquet").saveAsTable("dessert_tbl_parquet")
//format("parquet")를 테이블 이름 dessert_tbl_parquet으로 저장 후 조회
spark.read.format("parquet").table("dessert_tbl_parquet").show(3)
spark.sql("select*from dessert_tbl_parquet limit 3").show
```

#### 세이프 모드

- 출력 장소에 지정한 데이터셋이 이미 존재할 경우 어떻게 처리할지 결정

| 세이프 모드          | 효과                                                         |
| -------------------- | ------------------------------------------------------------ |
| SaveMode.ErrorExists | 예외를 발생시킨다(디폴트)                                    |
| SaveMode.Append      | 기존 데이터셋에 덧붙인다                                     |
| SaveMode.Overwrite   | 기존 데이터셋을 덮어쓴다.                                    |
| SaveMode.Ignore      | 기존 데이터셋을 변경하지 않는다.(=create tabe if not exists) |

- 세이프 모드 하려면 **org.apache.spark.sql.SaveMode**임포트 해야한다.

```cmd
hadoop fs -mkdir /output/dessert_json
```

```scala
dessertDF.write.save("/output/dessert_json") //예외 AnalysisException 발생 (이미 존재함)

```

```scala
import org.apache.spark.sql.SaveMode
dessertDF.write.format("json").mode(SaveMode.Overwrite).save("output/dessert_json") //예외발생 안한다!

val dessertDF2=dfReader.format("json").load("/output/dessert_json")
dessertDF2.orderBy($"kcal").show(4)
+------+---------------+-----+----+
|menuId|           name|price|kcal|
+------+---------------+-----+----+
|  D-15|  커스터드 푸딩| 2000| 120|
|   D-8|  바닐라 젤라또| 3600| 131|
|  D-17|    치즈 수플레| 2200| 160|
|  D-14|바나나 크레이프| 3300| 220|
+------+---------------+-----+----+
only showing top 4 rows

```



#### 명시적으로 스키마 정보 부여하기

- 데이터에 대한 스키마 정보를 나타내는 API, StuctType은 데이터프레임의 레코드에 대한 구조 정보를 나타내며, 내부에 여러 개의 StructField를 갖는 형태로 정의

```scala
import java.math.BigDecimal
case class DecimalTypeContainer(data: BigDecimal)
val bdContainerDF=sc.parallelize(
List(new BigDecimal("123456.6789999999999"))).map(data=>DecimalTypeContainer(data)).toDF
bdContainerDF.printSchema

bdContainerDF.show(false)//data문자열의 길이가 20을 넘을 경우에도 생략하지 않고 표시하려는 것
+--------------------+
|data                |
+--------------------+
|123456.6789999999999|
+--------------------+

bdContainerDF.write.format("orc").save("/output/bdContainerORC")
val bdContainerORCDF=spark.sqlContext.read.format("orc").load("/output/bdContainerORC")
bdContainerORCDF.printSchema
bdContainerORCDF.show(false)
+-------------------------+
|data                     |
+-------------------------+
|123456.678999999999900000|
+-------------------------+

bdContainerDF.write.format("json").save("/output/bdContainerJSON")
val bdContainerJSONDF= spark.sqlContext.read.format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema

bdContainerJSONDF.show(false)
+----------+
|data      |
+----------+
|123456.679|
+----------+

import org.apache.spark.sql.types.DataTypes._
val schema=createStructType(Array(createStructField("data",createDecimalType(38,18),true)))

val bdContainerJSONDF=spark.sqlContext.read.schema(schema).format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema

bdContainerJSONDF.show(false)
+-------------------------+
|data                     |
+-------------------------+
|123456.678999999999900000|
+-------------------------+
//이처럼 스키마를 명시적으로 설정함으로 형변환에 따른 오차 없이 파일 내용을 정확히 DataFrame으로 변환 가능
```

#### 파티셔닝

- where절이나 where 메서드의 필터링 조건식에 파티션을 지정하여 해당 파티션 이외의 데이터를 읽지 않게 함

```scala
import org.apache.spark.sql.types.DataTypes._
val priceRangeDessertDF=dessertDF.select(((($"price"/1000)cast IntegerType)*1000)as "price_range",dessertDF("*"))



//priceRangeDessertDF를 파티셔닝하지 않고 출력
priceRangeDessertDF.write.format("parquet").save("/output/price_range_dessert_parquet_non_partitioned")

val nonPartitionedDessertDF=spark.sqlContext.read.format("parquet").load("/output/price_range_dessert_parquet_non_partitioned")
nonPartitionedDessertDF.where($"price_range">=5000).explain
== Physical Plan ==
*(1) Project [price_range#243, menuId#244, name#245, price#246, kcal#247]
+- *(1) Filter (isnotnull(price_range#243) && (price_range#243 >= 5000))
   +- *(1) FileScan parquet [price_range#243,menuId#244,name#245,price#246,kcal#247] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/output/price_range_dessert_parquet_non_partitioned], PartitionFilters: [], PushedFilters: [IsNotNull(price_range), GreaterThanOrEqual(price_range,5000)], ReadSchema: struct<price_range:int,menuId:string,name:string,price:int,kcal:int>

```

```scala
//priceRangeDessertDF를 파티셔닝하고 출력
priceRangeDessertDF.write.format("parquet").partitionBy("price_range").save("/outputprice_range_dessert_parquet_partitioned")
val partitionedDessertDF=spark.sqlContext.read.format("parquet").load("/outputprice_range_dessert_parquet_partitioned")
partitionedDessertDF.where($"price_range">=5000).explain
                                                                                                     == Physical Plan ==
*(1) FileScan parquet [menuId#259,name#260,price#261,kcal#262,price_range#263] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/outputprice_range_dessert_parquet_partitioned], PartitionCount: 2, PartitionFilters: [isnotnull(price_range#263), (price_range#263 >= 5000)], PushedFilters: [], ReadSchema: struct<menuId:string,name:string,price:int,kcal:int>

```

#### 구조화된 데이터셋을 테이블로 다루기

`create table 테이블명 using <해당 포맷의 프로바이더명>options(옵션,옵션....)

- using절에는 다루고자 하는 포맷에 대응하는 프로바이더의 이름 지정 . 아래를 참고

| 파일 포맷 | 대응 프로바이더명             |
| --------- | ----------------------------- |
| Parquet   | org.apache.spark.sql.parquet  |
| ORC       | org.apache.spark.sql.hive.orc |
| JSON      | org.apache.spark.sql.json     |

```cmd
[hadoop@master ~]$ spark-sql
#하면 들어가진다 spark-sql
```

```scala
create table dessert_tbl_json USING org.apache.spark.sql.json OPTIONS(path 'output/dessert_json');

SELECT name, price FROM dessert_tbl_json LIMIT 3;
초콜릿 파르페   4900
푸딩 파르페     5300
딸기 파르페     5200

```

#### 테이블 캐시

- DataFrame이나 테이블을 이그제큐터에 캐시 가능. 

```scala
df.cache()// DataFrame을 캐시에 저장, df는 DataFrame을 나타낸다.
spark.sqlContext.cacheTable("tbl")//테이블을 캐시에 저장, sqlContext는 SQLContext의 인스턴스를 tbl은 테이블명을 나타낸다.

df.unpersist// DataFrame 캐시에서 삭제
spark.sqlContext.uncacheTable("tbl")//테이블을 캐시에서 삭제

```

## 스파크 스트리밍

#### 스트림처리

- 7장에서는 스트림처리를 구현하기 위해 스트리밍 라이브러리가 있다.

- 짧은 시간 발생한 데이터를 반복적으로 처리하는 것 

- 예를 들어 혈액냉장고의 온도 센서 확인 등

- 갑작스러운 변화의 경향 파악이 쉽다.

  

#### 스트림 데이터 란?

- 스토리지에 저장된 하나의 큰 데이터가 아닌 '반영구적으로 계속 생성되는 데이터'
- 짧은 시간 간격으로 생성되는 것이 특정

### 스파크 스트리밍

- RDD변환처리로 데이터처리 구현
- 조각을 나누어 입력 데이터를 구성, 반복 처리한다는 것이 특징

#### DStrema 변환

1. map
   - 원래의 DStream을 변환하여 새로운 DStream 생성하는 API다.
2. flatMap
   - DStream의 각 요소에 대해서 함수를 적용하고, 다차원 컬렉션을 한 차원 풀어 내린 요소들로 이루어진 DStream생성. 인수로는 변환처리가 정의된 함수를 건네준다.
3. filter
   - 원래의 DStream에서 조건을 만족하는 요소만을 남긴 DStream을 생성. 인수로는 조건을 확인하기 위한 함수를 건네준다.

#### 스파크 스트리밍 동작 확인

1. 

#### 넷캣(Netcat)

1. TCP나 UDP프로토콜을 사용하는 네트워크 연결에서 데이터를 읽고 쓰는 간단한 유틸리티 프로그램

   nc는 network connection에 읽거나 쓴다.

   Network connection에서 raw-data read,write를 할 수 있는 유틸리티프로그램으로 원하는 포트로 원하는 데이터를 주고받을 수 있는 특징으로  해킹에도 사용되며 컴퓨터 포렌식에 있어서 라이브시스템의 데이터를 손쉽게 가져오기 위해 사용

```cmd
#마스터에서
[hadoop@master ~]$ nc -l 9999
#슬레이브에서
[hadoop@slave ~]$ nc 마스터ipaddress(000.000.000.000) 9999
#하면 둘이 연결이 된다! 아무거나 작성해 보자

#마스터에서 (파일은 만들어 놓아야 한다.)
nc -l 9999 > ./listen.txt
#슬레이브에서
nc 192.168.255.130 9999 < ./input.txt


```

- 마스터에서 ` nc -lk 9999' 상태에서(slave연결 끊고)
- /usr/local/hadoop2.7.7/ 에서 spark-shell (--master local[*])실행 후

```scala
import org.apache.spark.{SparkContext,SparkConf}
import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level,Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc=new StreamingContext(sc,Seconds(10))
val lines=ssc.socketTextStream("localhost",9999,StorageLevel.MEMORY_AND_DISK_SER)
val words=lines.flatMap(_.split(" "))
val wordCounts=words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()


```

2. 클러스터 환경에 애플리케이션 배포

```scala
//hadoop fs -mkdir /data/sample_dir
import org.apache.spark.{SparkContext,SparkConf}
import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level,Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc=new StreamingContext(sc,Seconds(10))
val lines=ssc.textFileStream("/data/sample_dir/")
val words=lines.flatMap(_.split(" "))
val wordCounts=words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()

//hadoop fs -put /usr/local/spark/README.md /data/sample_dir
//대상 파일을 읽어서 wordCount처리 하는 중! 파일이 업로드 되면 바로 읽는다.
```

#### 평균데이터 계산

- 스파크 스트리밍을 이용해 샘플 데이터로 
- [데이터](http://archive.ics.uci.edu/ml/index.php) 에서 human activity ~를 data folder로 zip으로 받자

```cmd
[hadoop@master Downloads]$ unzip UCI\ HAR\ Dataset.zip
#파일이 이름에 공백이 있으므로 unzip으로 풀어주자.
[hadoop@master Downloads]$ mv UCI\ HAR\ Dataset UCI_HAR_Dataset
#공백이 있어 불편함으로 이름을 바꿔주자 UCI_HAR_Dataset으로 이름이 바뀐것을 확인한다.



```



## 머신러닝:MLlib

- MLlib은 통계 처리나 머신러닝을 구현하기 위한 라이브러리로, 내부적으로 스파크 코어의 기본 API를 이용하므로 분산처리 기능 자연스럽게 사용 가능.
- 스파크 코어에는 스파크 SQL, MLlib, 스파크 스트리밍, 그래프 X가 있다.

#### 통계 처리, 머신러닝

- 특정 데이터로부터 수학적 기법을 이용하여 그 성질을 끄집어내는 처리
- 과거의 데이터를 이용하여 미래 데이터에 대한 예측을 하는 처리
- 대부분 행렬계산 라이브러리
- 예로는 
  1. 텍스트 정보로부터 긍정적/부정적 표현 여부 판정
  2. 인터넷 쇼핑몰 등에서 사용자의 일정한 패턴을 부석하여 상품 추천
  3. 등등
- [스파크공식사이트](https://spark.apache.org/)에서 최신 정보가 추가 되고 있다.

#### 기본 요소

1. 벡터(Vector)- 1차원 데이터 다루는 데이터 타입

   - LocalVector

     > 기본적인 벡터 타입으로 밀집 벡터와 희소 벡터 클래스를 이용

   - LabeledPoint

     > LocalVector+레이블 
     >
     > 알고리즘이용시 독립변수와 종속 변수 함께 보존시 사용

2. 행렬(Matrix)-1개 이상의 벡터로 구성되는 행렬 형식 데이터 타입

   - LocalMatrix

     > 한 대의 호스트로 행렬 형식 데이터 다루기 위한 데이터 타입

   - DistributedMatrix

     > 대규모 데이터셋을 분산처리하기 위한 행렬 형식 데이터 타입.

#### 분류와 회귀

- 미리 학습해둔 학습 데이터를 통해 분류모델 작성, 따로 처리된 데이터에 모델 을 적용할 수 있다. 

#### 협업 필터링

- 고객/ 상품에 관한 구매 이력 등의 데이터를 입력데이터로 작성하고 이를 이용하여 관련 정보 추천을 한다.

#### 클러스터링

- 데이터가 주어졌을 때 여러 개의 집합으로 나누는 것. 예를 들어 정보성 메일을 받을 때 고객의 특성을 바탕으로 분류하고 이에 적합한 메일을 보내는 것

#### 차원 축소

- 차원 축소 방법으로 SVD,PCA를 이용 할 수 있다. 이를 이용하여 어떤 정보든 형태적으로 유지하면서 데이터 차원을 축소 가능
- 이해하기 쉬운 저차원으로 변환하기 위해, 차원이 너무 높이 알고리즘 분석이 어려운 경우 사용

#### 특징 추출/변환

- 문서 특징 추출하는 방법중 하나로 벡터화 방법으로 TF-IDF, Word2Vec를 사용 가능.
- 수치 데이터 뿐만이 아닌 텏트 데이터도 입력으로 사용가능

#### 빈발 패턴 마이닝

- FP-growth, 연관성 규칙 마이닝, 순차 패턴 마이닝 등을 사용 가능. 이를 이용하여 '빵을 사는 사람은 우유를 사는 경우가 많다' 와 같은 패턴 발견 가능. 

#### PMML 익스포트 기능

#### PMML익스포트 기능

- 스파크 이외의 툴에서 이 모델을 사용할 수 있게 하는 것.

#### K-means 개요

- K-means의 샘플용 데이터는 스파크 본체에 포함

```cmd
cd usr/local/spark
cd data/mllib
cat kmeans_data.txt
```

- 로 확인 가능

```scala
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
val sparkHome=sys.env("SPARK_HOME")
val data=sc.textFile("file://"+sparkHome+"/data/mllib/kmeans_data.txt")
val parsedData=data.map{s=>
Vectors.dense(s.split(' ').map(_.toDouble))}.cache()


val numClusters=2
val numIterations=20
val clusters=KMeans.train(parsedData,numClusters,numIterations)

clusters.k

clusters.clusterCenters

val vec1=Vectors.dense(0.3,0.3,0.3)
clusters.predict(vec1)

val vec2=Vectors.dense(8.0,8.0,8.0)
clusters.predict(vec2)

parsedData.foreach(vec=>
                  println(vec+"=>"+clusters.predict(vec)))

val predictedLabels=parsedData.map(vector=> clusters.predict(vector))
predictedLabels.saveAsTextFile("output/kmeans")
//경로 지정을 안했으므로 하둡에 /user/hadoop/output/kmenas에 있다.
clusters.save(sc, "kmeans_model")


```

```cmd
[hadoop@master ~]$ hadoop fs -cat /user/hadoop/kmeans_model/metadata/part-00000 
{"class":"org.apache.spark.mllib.clustering.KMeansModel","version":"2.0","k":2,"distanceMeasure":"euclidean","trainingCost":0.11999999999994547}
#현재 사용하는 모델의 조건? 등을 알 수 있다.
```



##### 만약 클러스터의 갯수를 모른다면

```scala
val WSSSE=clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors="+WSSSE) Within Set Sum of Squared Errors=0.11999999999994547
```

#### 단어의 벡터화(한국어)

##### 문서에서 단어 추출(형태소 분석)

- 한국어는 단순한 공백으로 분할로는 충분하지 않기에 의미를 갖는 최소한의 단위인 형태소로 분할해야 한다.
- 형태소 분석은 MLlib에 포함되어있지는 않다.
- twitter-korean-text를 이용한다.

[트위터메이븐 한글 자르파일](https://mvnrepository.com/artifact/com.twitter.penguin/korean-text/4.4.4) 다운!

```cmd
$spark_HOME/jars폴더에 mv
```



```scala
libraryDependencies += "com.twitter.penguin" %% "korean_text" % "4.0"
```

```scala
spark-shell --master yarn \
--packages com.twitter.penguin:korean-text:4.4.4

spark-shell \
--master yarn \
--packages com.twitter.penguin:korean-text:4.4.4 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
// 위에 \표시는 엔터 표시임으로 \ 지우고 한줄로 작성해도 된다spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
//안되면 yarn말고 local로 하자
import java.io.StringReader
import org.apache.spark.mllib.feature.{Word2Vec,Word2VecModel}
import com.twitter.penguin.korean.TwitterKoreanProcessor
import com.twitter.penguin.korean.tokenizer.KoreanTokenizer.KoreanToken
import org.apache.spark.mllib.linalg.Vectors

val sentence="이 책은 무슨 책 입니까"
val normalized: CharSequence=TwitterKoreanProcessor.normalize(sentence)

//토크나이즈
val tokens: Seq[KoreanToken]=TwitterKoreanProcessor.tokenize(normalized)

//어근 추출
val stemmed: Seq[KoreanToken]=TwitterKoreanProcessor.stem(tokens)
```





##### 실제 파일로 찾아보자

```scala
val input=sc.textFile("")
```



spark-shell --master local --packages com.twitter.penguin:korean-text:4.4.4 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer

spark-shell --master yarn --packages com.twitter.penguin:korean-text:4.4.4



## 스파크 Streaming

1. 단순히 주어진 데이터를 읽고 처리하는 것이 아닌 변화하는 데이터를 다루기 위한 것
2. 일정한 주기마다 새로운 RDD를 읽어와서 그 이전에 생성했던 RDD처리 결과와 혼합해서 필요한 처리를 수행하는 행위를 애플리케이션이 종료될 때까지 무한히 반복하는 형태로 동작.

```scala
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}
import scala.collection.mutable.Queue

val conf = new SparkConf() .setMaster("local[*]").setAppName("SteamingSample")    
val sc = new SparkContext(conf)    
val ssc = new StreamingContext(sc, Seconds(3))    

val rdd1 = sc.parallelize(List("Spark Streaming Sample ssc"))    
val rdd2 = sc.parallelize(List("Spark Quque Spark API"))   //들어오는 RDD데이터를 Quque에 넣어놓는다. 
val inputQueue = Queue(rdd1, rdd2)    
val lines = ssc.queueStream(inputQueue, true)    //queue로부터 
val words = lines.flatMap(_.split(" "))   
 words.countByValue().print()    //value값 기본으로 count
ssc.start()    //straming 를 시작한것
ssc.awaitTermination //일정주기간격으로 반복 수행 할 것 

```

1. 스파크 Streaming

- 스트리밍 컨텍스트 (StreamingContext)는 명시적인 시작(start)과 종료(stop), 대기(awaitTermination) 메서드를 사용해야 한다.
- 스트리밍 애플리케이션의 특성상 명시적인 종료 메시지나 에러가 없다면 애플리케이션이 임의로 종료되지 않아야 하므로 start()를 호출한 다음 awaitTermination()를 호출해 애플리케이션이 종료되지 않게 해야 한다.
- 스트리밍 컨텍스트는 단 한번 시작되고 종료된다. (한번 종료한 스트리밍 컨텍스트를 다시 재시작할 수 없습니다.)
- 스트리밍 컨텍스트는 한번 시작되고 나면 더 이상 새로운 연산을 정의하거나 추가할 수 없다.
- JVM당 오직 하나의 스트리밍 컨텍스트만 동시에 활성화될 수 있다.
- 스트리밍 컨텍스트의 stop() 메서드를 호출하면 연관된 SparkContext도 함께 중지
- 스트리밍 컨텍스트만 종료하고 싶다면 stop() 메서드의 stopSparkContext 매개변수 값을 false로 지정
- 한 번에 하나의 스트리밍 컨텍스트만 동작한다는 가정하에 하나의 스파크 컨텍스트로부터 여러 개의 스트리밍 컨텍스트를 생성가능

2. Dstream(Discretized Stream)

- 고정되지 않고 끊임없이 생성되는 연속된 데이터를 나타내기 위한 추상 모델
- 데이터스트림을 처리해서 일정 시간마다 데이터를 모아서 RDD를 만드는데 이러한 RDD로 구성된 시퀀스
- 마지막 데이터를 읽어들인 시점으로부터 배치 간격에 해당 하는 시간 동안 새로 생성된 데이터를 읽어들여 새로운 RDD를 생성

3. 기본 데이터 소스

- 외부 라이브러리의 도움 없이 스파크 단독으로 지원 가능
- 소켓, 파일, RDD 큐어

4. 어드밴스드 데이터 소스

- 카프카(Kafka), 플럼(Flume), 키니시스(Kinesis), 트위터(Twitter)등과 연동해서 사용 가능
- 스파크가 제공하는 Receiver 추상 클래스를 상속받아 사용자 정의 데이터 소스를 사용할 수 있습니다.



#### 카프카

```scala
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}
object KafkaSample {
  def main(args: Array[String]) {    
    import org.apache.spark.streaming.kafka._
    val conf = new SparkConf()
      .setMaster("local[*]")
      .setAppName("KafkaSample")
    val sc = new SparkContext(conf)
    val ssc = new StreamingContext(sc, Seconds(3))
    val zkQuorum = "localhost:2181"
    val groupId = "test-consumer-group1"
    val topics = Map("test" -> 3)    
    val ds1 = KafkaUtils.createStream(ssc, zkQuorum, groupId, topics)//데이터 수신 받고
    val ds2 = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, 
        Map("metadata.broker.list" -> "localhost:9092"), //오류생기면 번만 
        Set("test"))//객체 자동으로 생성
    ds1.print
    ds2.print
    ssc.start//명시적인 시작
    ssc.awaitTermination()//종료되지 않도록 (명시적 stop메서드도 쓸 수 있다.
  }
}

```

5. 데이터 기본 연산

- print() – DStream에 포함된 RDD의 내용을 콘솔에 출력
- map(func) - DStream에 포함된 RDD의 각 원소에 func 함수를 적용한 결과값으로 구성된 새로운 DStream을 반환
- flatMap(func) - DStream에 포함된 RDD의 각 원소에 func 함수를 적용한 결과값으로 구성된 새로운 DStream을 반환, 하나의 입력이 0~N개의 출력으로 변환
- count(), countByValue() - DStream에 포함된 요소의 개수를 DStream 으로 반환
- reduce(func), reduceByKey(func) - DStream에 포함된 RDD 값을 집게해서 최종적으로 하나의 값으로 DStream 으로 반환
- Filter(func) - DStream에 포함된 요소에 func 함수를 적용한 결과가 true인 요소만 포함한 새로운 DStream 으로 반환// true인지 false인지에서 true만 포함
- union()  - 두개의 DStream의 요소를 모두 포함한 새로운 DStream  생성
- join() – 키와 값 쌍으로 구성된 두 개의 DStream을 키를 이용해 조인, leftOuterJoin(), rightOuterJoin(), fullOuterJoin()

6. 데이터 고급 연산

- transform(func) – DStream에 내부의 RDD에 func 함수를 적용하고 그 결과로 새로운 DStream을 반환
- updateStateByKey() – 배치가 실행될 때마다 새로 생성된 데이터와 이전 배치의 최종 상태값을 함께 전달해주기 때문에 각 키별 최신 값, 즉 상태(state)를 유지하고 갱싱할 수 있습니다. (새로운값과 유지된 값을 같이 전달하기 위해서 하나를 유지시켜야 한다. 즉 상태 유지하고 갱싱 )
- ckeckpoint() – 현재의 작업 상태를 HDFS와 같은 영속성을 가진 저장소에 저장 

```scala
val ssc = new StreamingContext(conf, Seconds(3))
val t1 = ssc.sparkContext.parallelize(List("a", "b", "c"))
val t2 = ssc.sparkContext.parallelize(List("b", "c"))
val t3 = ssc.sparkContext.parallelize(List("a", "a", "a"))
val q6 = mutable.Queue(t1, t2, t3)
val ds6 - ssc.queueStream(q6, true)
ssc.checkpoint(".")// checkpoint 이유는 메모리에 저장하면 안되는 것이 휘발성이기 때문에 변경된 데이터를 일정시간마다 디스크에 쓰는 작업을 checkpoint라 한다. 장애대비
val updateFunc = (newValues: Seq[Long], currentValue: Option[Long]) => Option(currentValue, getOrElse(0L) + newValues.sum)
ds6.map((_, 1)).updateStateByKey(updateFunc).print 

```



7. 윈도우 연산

- StreamingContext는 정해진 주기마다 새로 생성된 데이터를 읽어서 RDD를 생성하며, 생성된 RDD는 DStream이 제공하는 API를 이용해 처리할 수 있습니다.
- 스트리밍 데이터의 가장***마지막에 수행된 배치의 결과 뿐 아니라 이전에 수행된 배치의 결과까지 함께 사용***해야 하는 경우, ***윈도우 연산을 활용***할 수 있습니다.
- 윈도우 연산은 마지막 배치가 수행됐을 때 읽어온 데이터뿐 아니라 그 이전에 수행된 배치의 입력 데이터까지 한꺼번에 처리할 수 있도록 지원하는 연산 (updateStateByKey()와 다른 점은 마지막과 새로운 것만 처리하는 것이  update라면 이전의 이전까찌 가져오는 것이 윈도우 연산)
- 윈도우 연산은 수행하기 위해서는 얼마만큼의 간격으로 윈도우 연산을 수행할 것인지와 한번 수행할 때 얼마만큼의 과거 배치 수행 결과를 가져올 것인지에 대한 정보를 지정해야 합니다.
- spark의 연산은 여러 배치 들의 결과를 합쳐서 의 배치 간격보다 훨씬 긴 시간 간격에 대한 결과를 계산한다

- window(windowLength, slideInterval) - slideInterval에 지정한 시간마다 windowLength에 지정한 크기만큼의 시간 동안 발생된 데이터를 포함한 DStream을 생성합니다.
- countByWindow(windowLength, slideInterval) – 윈도우에 포함된 요소의 개수를 포함한 DStream을 생성합니다.   
- reduceByWindow(func, windowLength, slideInterval) - 윈도우에 포함된 요소에 reduce() 함수를 적용한 결과로 구성된 DStream을 생성합니다.

```scala
var conf = new SparkConf().setMaster("local[*]").setAppName("WindowSample")
val sc = new SparkContext(conf)
val ssc = new StreamingContext(sc, Seconds(1))
ssc.checkpoint(".")
val input = for (i <- mutable.Queue(1 to 100: _*)) yield sc.parallelize(i :: Nil)
val ds = ssc.queueStream(input)
ds.window(Second(3), Seconds(2)).print

ds.countByWindow(Second(3), Second(2)).print

ds.reduceByWindow( (a, b) => Math.max(a, b), Seconds(3), Second(2)).print

```



```scala
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import scala.collection.mutable
//1
val ssc = new StreamingContext(sc, Seconds(1))
ssc.checkpoint(".")
val input = for (i <- mutable.Queue(1 to 100: _*)) yield sc.parallelize(i :: Nil)
val ds = ssc.queueStream(input)
ds.window(Seconds(3), Seconds(2)).print //배치처리 3개를 가져오는데 2초마다 가져오게 된다. 새로운것과 이전것까찌해서 3개!
ds.countByWindow(Seconds(3), Seconds(2)).print
ssc.start()
ssc.awaitTermination()
//2
val ssc = new StreamingContext(sc, Seconds(1))
ssc.checkpoint(".")
val input = for (i <- mutable.Queue(1 to 100: _*)) yield sc.parallelize(i :: Nil)
val ds = ssc.queueStream(input)
ds.reduceByWindow( (a, b) => Math.max(a, b), Seconds(3), Seconds(2)).print
ssc.start()

//3
val ssc = new StreamingContext(sc, Seconds(1))
ssc.checkpoint(".")
val input = for (i <- mutable.Queue(1 to 100: _*)) yield sc.parallelize(i :: Nil)
val ds = ssc.queueStream(input)
ds.map( v => (v%2, 1)).reduceByKeyAndWindow((a: Int, b: Int) => a+b, Seconds(4), Seconds(2)).print
ssc.start()
//4
val ssc = new StreamingContext(sc, Seconds(1))
ssc.checkpoint(".")
val input = for (i <- mutable.Queue(1 to 100: _*)) yield sc.parallelize(i :: Nil)
val ds = ssc.queueStream(input)
ds.countByValueAndWindow(Seconds(3), Seconds(2)).print
ssc.start()


```



## 스파크 MLlib

1. 관측과 특성
   머신러닝에서 특성(feature)는 관측(Observation) 데이터의 속성을 나타내는 용도로 사용됩니다.
   원본 데이터로부터 특성을 추출하는 과정은 데이터의 변환, 필터링, 정규화, 특성 간 상관 관계 분석 등 다양한 작업을 포함할 수 있습니다.
   스파크 Mllib 에서는 특성 추출 작업을 더욱 편리하게 수행할 수 있는 다양한 특성 추출 및 변환, 선택 알고리즘과 유틸리티 함수를 제공합니다.
2. 지도학습
   입력에 대한 올바른 출력 값을 알고 있는 데이터셋을 가지고 입력과 그에 따른 출력 값을 함께 학습하게 한 뒤 아직 답이 알려지지 않은 새로운 입력값에 대한 출력값을 찾게 하는 방법
   훈련을 위해 주어지는 데이터 셋에 각 관측 데이터에 대한 올바른 출력값을 알려주는 레이블(Label)이라는 값이 포함됩니다.
   스파크에서는 레이블을 포함한 데이터셋을 다루기 위해 LabeledPoint라는 데이터 타입을 사용합니다.
3. 연속성 데이터 (Continuous Data)
   무게나 온도, 습도와 같이 연속적인 값을 가지는 데이터
   실수값
4. 이산형 데이터(Discrete data)
   나이나 성별, 사과의 개수 등과 같이 불연속적인 값을 가지는 데이터
5. 정수나 문자값
   스파크 MLlib에서 제공하는 API는 연속성 데이터 , 이산형 데이터의 입력 및 출력 데이터 모두 double 타입의 데이터만 사용할 수 있습니다.

6. 모델
   알고리즘의 산출물로서 알고리즘에 데이터를 적용해서 만들어낸 결과물.
   머신러밍의 궁극적인 목적은 입력 데이터로부터 원하는 결과값, 출력 데이터를 얻어내는 것입니다.

7. Parametric 방식
   파라메트릭 알고리즘은 고정된 개수의 파라미터, 즉 계수를 사용하는 것으로 입력과 출력 사이의 관계를 특성 값에 관한 수학적 함수 또는 수식으로 가정하고, 이 수식의 결과가 실제 결과값에 가깝도록 계수를 조정하는 방법을 사용합니다.
   예) 실제 값과 예측 값 사이의 오차를 나타내는 손실함수(Loss function) 또는 비용함수(Cost Function)를 정의해서 이 함수값을 최소화하는 최적의 기울기와 계수을 찾아내는 방식의 선형회귀나 로지스틱회귀 알고리즘

8. Nonparametric 방식
   입력과 출력 사이의 가설을 세우지 않고 머신러닝의 수행 결과를 그대로 사용하는 방식
   SVM, Naïve Bayes 알고리즘

9. 지도학습(Supervised Learning)
   훈련 데이터에 레이블, 즉 정답에 관한 정보가 포함되며 알고리즘은 입력과 출력에 대한 가설과 정답 정보를 이용해 오차를 계산하고 이를 통해 입력과 출력 사이의 관계를 유추하게 됩니다.

   회귀(regression)과 분류(classification) 알고리즘
   훈련용으로 사용하는 모든 데이터에 레이블 정보를 추가해야 합니다.

10. 비지도학습(Unsupervised Learning)
    특성과 레이블 간의 인과 관계를 모르거나 특별히 지정하지 않고 컴퓨터의 처리에 맡기는 것
    군집(Clustering) 알고리즘

11. MLlib API
    spark.mllib 패키지를 사용하는 RDD 기반 API
    spark.ml 패키지는 사용하는 데이터 프레임 기반 API
12. MLlib API 기능
    머신러닝 알고리즘 – classification, regression, clustering, collaborative filtering등 알고리즘 제공
    특성 추출, 변환, 선택
    파이프라인 – 여러 종류의 머신러닝 알고리즘을 순차적으로 수행할 수 있는 API제공
    저장 – 알고리즘 모델, 파이프라인에 대한 저장 및 불러오기 기능을 제공
    유틸리티 – 선형대수, 통계, 데이터 처리 등의 유용한 함수를 제공

### 벡터

- 프로그램 상에서 double 타입의 값들을 포함하는 컬렉션으로 구현되며 벡터에 포함된 각 데이터는 정의된 순서에 따라 0부터 시작하는 정수형 인덱스를 부여 받습니다.
- org.apache.spark.ml.linalg 패키지에 정의된 트레이트
- Vector 인스턴스를 만들기 위해서는 값에 대한 정보만 가지고 있는 DenseVector 클래스나 값에 대한 인덱스 정보를 모두 가지고 있는 SparseVector 클래스 중 하나를 선택해서 해당 클래스의 인스턴스를 생성해야 합니다.
- desnse(), sparse() – 팩토리 메서드

```scala
import org.apache.spark.ml.linalg.Vectors
val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
val v2 = Vectors.dense(Array(0.1, 0.0, 0.2, 0.3))
val v3 = Vectors.sparse(4, Seq((0, 0.1), (2, 0.2), (3, 0.3)))
                        
val v4 = Vectors.sparse(4, Array(0, 2, 3), Array(0.1, 0.2, 0.3))

println(v1.toArray.mkString(", "))
println(v3.toArray.mkString(", "))

```

```scala

scala> import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.Vectors

scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]

scala> val v2 = Vectors.dense(Array(0.1, 0.0, 0.2, 0.3))
v2: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]

scala> val v3 = Vectors.sparse(4, Seq((0, 0.1), (2, 0.2), (3, 0.3)))
v3: org.apache.spark.ml.linalg.Vector = (4,[0,2,3],[0.1,0.2,0.3])
//요소 4개인데 0,2,3에 값이 있고 그 값은 0.1,0.2,0.3이라는 뜻이다.
//sparse는 희소행렬로 배열 대부분이 0인것
//dense는 밀집행렬로 배열 대부분이 0이 아닌 것
scala> val v4 = Vectors.sparse(4, Array(0, 2, 3), Array(0.1, 0.2, 0.3))
v4: org.apache.spark.ml.linalg.Vector = (4,[0,2,3],[0.1,0.2,0.3])

scala>

scala> println(v1.toArray.mkString(", "))
0.1, 0.0, 0.2, 0.3

scala> println(v3.toArray.mkString(", "))
0.1, 0.0, 0.2, 0.3

scala> println(v4.toArray.mkString(", "))
0.1, 0.0, 0.2, 0.3

```



##### LabeledPoint

- 레이블을 사용하는 경우를 위한 벡터로서 특성 값들을 담고 있는 벡터와 레이블 정보로 구성, 레이블에는 double 타입의 값만 할당 가능, 로지스틱 회귀와 같은 이진 분류 알고리즘을 사용할 경우 0(negative)나 1(position)로 설정

```scala
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.Vectors

val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
val v5 = LabeledPoint(1.0, v1)

println(s"label:${v5.label}, features:${v5.features}")
```



```scala
scala> import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.feature.LabeledPoint

scala> import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.Vectors

scala> val v1 = Vectors.dense(0.1, 0.0, 0.2, 0.3)
//
v1: org.apache.spark.ml.linalg.Vector = [0.1,0.0,0.2,0.3]

scala> val v5 = LabeledPoint(1.0, v1)
v5: org.apache.spark.ml.feature.LabeledPoint = (1.0,[0.1,0.0,0.2,0.3])
//LabeledPoint : 이것은 앞서 말한 입력값(feature)과 그에 따른 올바른 출력값(label)의 쌍을 만들어주는, 스파크가 제공하는 데이터 타입으로 (입력값, 올바른 출력값) 쌍으로 이루어져 있는 데이터 타입이며, 이 (feature, label) 쌍을 LabeledPoint 라고 한다.
scala> println(s"label:${v5.label}, features:${v5.features}")
label:1.0, features:[0.1,0.0,0.2,0.3]

```

##### 파일을 이용한 SparseVecotr 생성

```scala
//파일을 이용한 SparseVector  생성
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.mllib.util.MLUtils

val path = "file:///usr/local/spark/data/mllib/sample_libsvm_data.txt"
val v6 = MLUtils.loadLibSVMFile(sc,  path)
val lp1 = v6.first
//첫번째 줄만 가져와서 확인 출력량이 많이 때문에
 println(s"label:${lp1.label}, features:${lp1.features}")

```

```scala
// 값 확인
scala> val path = "file:///usr/local/spark/data/mllib/sample_libsvm_data.txt"
path: String = file:///usr/local/spark/data/mllib/sample_libsvm_data.txt

scala> val v6 = MLUtils.loadLibSVMFile(sc,  path)
//path는 위의 변수인데 sc....????
//scala> print(sc) -- 스파크 실행시 자동생성하는 것!!!
//org.apache.spark.SparkContext@19a7e618 이거다 언제 넣은 것인가!?

v6: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86

scala> val lp1 = v6.first
//맨앞의 0.0이 라벨이다.
lp1: org.apache.spark.mllib.regression.LabeledPoint = (0.0,(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50....
//[51부터는 value다. 그후 ...은 양이 많아 줄인것
//첫번째 줄만 가져와서 확인 출력량이 많이 때문에

scala>  println(s"label:${lp1.label}, features:${lp1.features}")
label:0.0, features:(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0])

```

##### 파이프라인

- 여러 가지의 알고리즘을 순차적으로 실행시킬 때 사용하며 머신러닝을 위한 워크 플로우 생성 가능
- 머신러닝은 데이터 수집부터, 가공, 특성 추출, 알고리즘 적용 및 모델 생성, 평가, 배포 및 활용에 이르는 일련의 작업을 반복하며 수행됩니다.

- 파이프라인은 여러 종류의 알고리즘을 순차적으로 실행할 수 있게 지원하는 고차원 API이며, 파이프 라인 API를 이용해 머신러닝을 위한 워크 플로우를 생성할 수 있습니다.

- 파이프라인은 데이터 프레임을 사용합니다.

- **Transformer** – org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임을 변형해 새로운 데이터프레임을 생성하는 용도로 사용

- **Estimator(평가)** - org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머를 생성하는 역할을 합니다.

- **Pipeline** - org.apache.spark.ml 패키지에 선언된 클래스. 여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자. 하나의 파이프라인은 여러 개의 파이프라인 스테이지

  (PipelineStage)로 구성되며, 등록된 파이프라인 스테이지들은 *우선순위*에 따라 순차적으로 실행됩니다.

- **ParamMap** : 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스

```scala
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.VectorAssembler
//import org.apache.spark.sql.SparkSession
//object PipelineSample {  
//   def main(args: Array[String]) {    
//val spark = SparkSession.builder().appName("PipelineSample") .master("local[*]").getOrCreate()

// 훈련용 데이터 (키, 몸무게, 나이, 성별)
val training = spark.createDataFrame(Seq(      (161.0, 69.87, 29, 1.0),      (176.78, 74.35, 34, 1.0),      (159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender") 
training.cache() 
   // 테스트용 데이터    
val test = spark.createDataFrame(Seq(      (169.4, 75.3, 42),      (185.1, 85.0, 37),      (161.6, 61.2, 28))).toDF("height", "weight", "age")    
training.show(false)    
val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")    
// training 데이터에 features 컬럼 추가    
val assembled_training = assembler.transform(training)    
assembled_training.show(false)    
// 모델 생성 알고리즘 (로지스틱 회귀 평가자)    
val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01) .setLabelCol("gender") 
// 모델 생성    
val model = lr.fit(assembled_training)    
// 예측값 생성    
model.transform(assembled_training).show()    
// 파이프라인    
val pipeline = new Pipeline().setStages(Array(assembler, lr))    
// 파이프라인 모델 생성    
val pipelineModel = pipeline.fit(training)    
// 파이프라인 모델을 이용한 예측값 생성    
pipelineModel.transform(training).show()    
val path1 = "/Users/beginspark/Temp/regression-model"    
val path2 = "/Users/beginspark/Temp/pipelinemodel"    
// 모델 저장    
model.write.overwrite().save(path1)    
pipelineModel.write.overwrite().save(path2)    
// 저장된 모델 불러오기   
 val loadedModel = LogisticRegressionModel.load(path1)    
val loadedPipelineModel = PipelineModel.load(path2)    
spark.stop  
```

```scala
//결과값
 //object PipelineSample {
// def main(args: Array[String]) {
 //val spark = SparkSession.builder().appName("PipelineSample") .master("local[*]").getOrCreate()
// 훈련용 데이터 (키, 몸무게, 나이, 성별) 이 부분은 없어도 나오기에 제외한것!

scala> val training = spark.createDataFrame(Seq(      (161.0, 69.87, 29, 1.0),      (176.78, 74.35, 34, 1.0),      (159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender")
training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 2 more fields]

scala> training.cache()
res4: training.type = [height: double, weight: double ... 2 more fields]

// 테스트용 데이터

scala> val test = spark.createDataFrame(Seq(      (169.4, 75.3, 42),      (185.1, 85.0, 37),      (161.6, 61.2, 28))).toDF("height", "weight", "age")
test: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 1 more field]

scala> training.show(false)
+------+------+---+------+
|height|weight|age|gender|
+------+------+---+------+
|161.0 |69.87 |29 |1.0   |
|176.78|74.35 |34 |1.0   |
|159.23|58.32 |29 |0.0   |
+------+------+---+------+


scala> val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")
assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_07e34d911760

// training 데이터에 features 컬럼 추가

scala> val assembled_training = assembler.transform(training)
assembled_training: org.apache.spark.sql.DataFrame = [height: double, weight: double ... 3 more fields]

scala> assembled_training.show(false)
+------+------+---+------+-------------------+
|height|weight|age|gender|features           |
+------+------+---+------+-------------------+
|161.0 |69.87 |29 |1.0   |[161.0,69.87,29.0] |
|176.78|74.35 |34 |1.0   |[176.78,74.35,34.0]|
|159.23|58.32 |29 |0.0   |[159.23,58.32,29.0]|
+------+------+---+------+-------------------+


scala> // 모델 생성 알고리즘 (로지스틱 회귀 평가자-이진분류 알고리즘, 0아니면 1로)

scala> val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01) .setLabelCol("gender")
lr: org.apache.spark.ml.classification.LogisticRegression = logreg_604703782b71
//setMaxIter 반복 횟수 setRegParam정규화를 위한 메소드, setLabelCol은 라벨
// 모델 생성

//fit 은 트레이니 데이터를 통해 모델을 생성하는 것이다.
scala> val model = lr.fit(assembled_training)
19/09/05 09:36:50 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
19/09/05 09:36:50 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_604703782b71, numClasses = 2, numFeatures = 3

// 예측값 생성
//모델로 부터 예측값을 생성하는 것
scala> model.transform(assembled_training).show()

+------+------+---+------+-------------------+--------------------+--------------------+----------+
|height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
+------+------+---+------+-------------------+--------------------+--------------------+----------+
| 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
|176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
|159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
+------+------+---+------+-------------------+--------------------+--------------------+----------+


// 파이프라인
//파이프라인을 이용하여 스테이지를 만드는 것!                                   

scala> val pipeline = new Pipeline().setStages(Array(assembler, lr))
pipeline: org.apache.spark.ml.Pipeline = pipeline_fb1773d0d609

// 파이프라인 모델 생성
//fit 메서드를 이용해 모델 생성
scala> val pipelineModel = pipeline.fit(training)
pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_fb1773d0d609

// 파이프라인 모델을 이용한 예측값 생성

scala> pipelineModel.transform(training).show()
+------+------+---+------+-------------------+--------------------+--------------------+----------+
|height|weight|age|gender|           features|       rawPrediction|         probability|prediction|
+------+------+---+------+-------------------+--------------------+--------------------+----------+
| 161.0| 69.87| 29|   1.0| [161.0,69.87,29.0]|[-2.4890615171055...|[0.07662857486628...|       1.0|
|176.78| 74.35| 34|   1.0|[176.78,74.35,34.0]|[-1.5515034131417...|[0.17486923465734...|       1.0|
|159.23| 58.32| 29|   0.0|[159.23,58.32,29.0]|[2.48077740707283...|[0.92278320971457...|       0.0|
+------+------+---+------+-------------------+--------------------+--------------------+----------+


scala> val path1 = "/Users/beginspark/Temp/regression-model"
//저장 경로를 바꿔 보자 /output/sparkmllib/regression-model                                                            
path1: String = /Users/beginspark/Temp/regression-model

scala> val path2 = "/Users/beginspark/Temp/pipelinemodel"
//output/sparkmllib/pipelinemodel                              
path2: String = /Users/beginspark/Temp/pipelinemodel

 // 모델 저장
//하둡에 저장되어 있으며 저장경로 output/sparkmllib/regression-model/data/part-00000-fb675073-8099-4da1-a18f-9c2c7e7a6c70-c000.snappy.parquet 아래 data디렉토리가 새로 생겼고 그 안에 정보 저장이 되어 있다.
                                                                    
scala> model.write.overwrite().save(path1)
scala> pipelineModel.write.overwrite().save(path2)
 // 저장된 모델 불러오기

scala>  val loadedModel = LogisticRegressionModel.load(path1)
loadedModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_604703782b71, numClasses = 2, numFeatures = 3

scala> val loadedPipelineModel = PipelineModel.load(path2)
loadedPipelineModel: org.apache.spark.ml.PipelineModel = pipeline_fb1773d0d609

```

```java
//##########자바 코드 예###########################
import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.classification.LogisticRegressionModel;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import java.util.Arrays;
import java.util.List;
public class PipelineSample {
public static void main(String[] args) throws Exception {
    SparkSession spark = SparkSession.builder()
            .appName("PipelineSample")
            .master("local[*]")
 .getOrCreate();
   StructField sf1 = DataTypes.createStructField("height", DataTypes.DoubleType, true);
   StructField sf2 = DataTypes.createStructField("weight", DataTypes.DoubleType, true);
   StructField sf3 = DataTypes.createStructField("age", DataTypes.IntegerType, true);
   StructField sf4 = DataTypes.createStructField("label", DataTypes.DoubleType, true);
  StructType schema1 = DataTypes.createStructType(Arrays.asList(sf1, sf2, sf3, sf4));
    List<Row> rows1 = Arrays.asList(RowFactory.create(161.0, 69.87, 29, 1.0),
        RowFactory.create(176.78, 74.35, 34, 1.0),
            RowFactory.create(159.23, 58.32, 29, 0.0));
    // 훈련용 데이터 (키, 몸무게, 나이, 성별)
    Dataset<Row> training = spark.createDataFrame(rows1, schema1);
training.cache();
List<Row> rows2 = Arrays.asList(RowFactory.create(169.4, 75.3, 42),
            RowFactory.create(185.1, 85.0, 37),
            RowFactory.create(161.6, 61.2, 28));
    StructType schema2 = DataTypes.createStructType(Arrays.asList(sf1, sf2, sf3));
    // 테스트용 데이터
    Dataset<Row> test = spark.createDataFrame(rows2, schema2);
   training.show(false);
   VectorAssembler assembler = new VectorAssembler();
    assembler.setInputCols(new String[]{"height", "weight", "age"});
   assembler.setOutputCol("features");
   Dataset<Row> assembled_training = assembler.transform(training);
  assembled_training.show(false);
// 모델 생성 알고리즘 (로지스틱 회귀 평가자)
 LogisticRegression lr = new LogisticRegression();
 lr.setMaxIter(10).setRegParam(0.01);
 // 모델 생성
   LogisticRegressionModel model = lr.fit(assembled_training);
 // 예측값 생성
 model.transform(assembled_training).show();
    // 파이프라인
   Pipeline pipeline = new Pipeline();
    pipeline.setStages(new PipelineStage[]{assembler, lr});
    // 파이프라인 모델 생성
   PipelineModel pipelineModel = pipeline.fit(training); 
    // 파이프라인 모델을 이용한 예측값 생성
    pipelineModel.transform(training).show();
    String path1 = "/output/sparkmllib/regression-model" ;
    String path2 = "/output/sparkmllib/pipelinemodel";
    // 모델 저장
   model.write().overwrite().save(path1);
   pipelineModel.write().overwrite().save(path2);
    // 저장된 모델 불러오기
  LogisticRegressionModel loadedModel = LogisticRegressionModel.load(path1);
  PipelineModel loadedPipelineModel = PipelineModel.load(path2);
    spark.stop();
  }
}


```

```python
##############Python 코드 예##########################
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.pipeline import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.sql import SparkSession
spark = SparkSession
.builde
.appName("pipeline_sample")
.master("local[*]")
.getOrCreate()
# 훈련용 데이터 (키, 몸무게, 나이, 성별)
training = spark.createDataFrame([
  (161.0, 69.87, 29, 1.0),
 (176.78, 74.35, 34, 1.0),
(159.23, 58.32, 29, 0.0)]).toDF("height", "weight", "age", "gender")
training.cache()
# 테스트용 데이터
test = spark.createDataFrame([
(169.4, 75.3, 42),
 (185.1, 85.0, 37),
(161.6, 61.2, 28)]).toDF("height", "weight", "age")
training.show(truncate=False)
assembler = VectorAssembler(inputCols=["height", "weight", "age"], outputCol="features")
# training 데이터에 features 컬럼 추가
assembled_training = assembler.transform(training)
assembled_training.show(truncate=False)
# 모델 생성 알고리즘 (로지스틱 회귀 평가자)
lr = LogisticRegression(maxIter=10, regParam=0.01, labelCol="gender")
# 모델 생성
model = lr.fit(assembled_training)
# 예측값 생성
model.transform(assembled_training).show()
# 파이프라인
pipeline = Pipeline(stages=[assembler, lr])
# 파이프라인 모델 생성
pipelineModel = pipeline.fit(training)
# 파이프라인 모델을 이용한 예측값 생성
pipelineModel.transform(training).show()
 
  path1 = "/output/sparkmllib/regression-model" 
  path2 = "/output/sparkmllib/pipelinemodel" 
# 모델 저장
model.write().overwrite().save(path1)
pipelineModel.write().overwrite().save(path2)
# 저장된 모델 불러오기
loadedModel = LogisticRegressionModel.load(path1)
loadedPipelineModel = PipelineModel.load(path2)
spark.stop
```

- 세 개의 코드를 비교해보자~

##### Tokenizer(문자열 처리, 공백문자를 기준으로)

- Tokenizer – 공백 문자를 기준으로 입력 문자열을 개별 단어의 배열로 변환하고 이 배열을 값으로 하는 새로운 컬럼을 생성하는 트랜스포머. 문자열을 기반으로 하는 특성 처리에 자주 사용됨
- RegexTokenizer – 정규식을 사용하여 문자열을 기반으로 하는 특성 처리  

```scala
import org.apache.spark.ml.feature.Tokenizer
import org.apache.spark.sql.SparkSession

Object TokenizerSample {  
   def main(args: Array[String]) {    
       val spark = SparkSession .builder() .appName("TokenizerSample") .master("local[*]") .getOrCreate()    
       val data = Seq("Tokenization is the process", "Refer to the Tokenizer").map(Tuple1(_))    
       val inputDF = spark.createDataFrame(data).toDF("input")    
       val tokenizer = new Tokenizer().setInputCol("input").setOutputCol("output")   
       val outputDF = tokenizer.transform(inputDF)    
           outputDF.printSchema()    
           outputDF.show(false)    
          spark.stop  
    }
}

```

```scala
scala> val spark = SparkSession .builder() .appName("TokenizerSample") .master("local[*]") .getOrCreate()
19/09/05 10:19:21 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4b5b7cbf

scala> val data = Seq("Tokenization is the process", "Refer to the Tokenizer").map(Tuple1(_))
data: Seq[(String,)] = List((Tokenization is the process,), (Refer to the Tokenizer,))

scala> val inputDF = spark.createDataFrame(data).toDF("input")
inputDF: org.apache.spark.sql.DataFrame = [input: string]

scala> val tokenizer = new Tokenizer().setInputCol("input").setOutputCol("output")
tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_70ab75b53890

scala> val outputDF = tokenizer.transform(inputDF)
outputDF: org.apache.spark.sql.DataFrame = [input: string, output: array<string>]

scala> outputDF.printSchema()
root
 |-- input: string (nullable = true)
 |-- output: array (nullable = true)
 |    |-- element: string (containsNull = true)


scala> outputDF.show(false)
+---------------------------+--------------------------------+
|input                      |output                          |
+---------------------------+--------------------------------+
|Tokenization is the process|[tokenization, is, the, process]|
|Refer to the Tokenizer     |[refer, to, the, tokenizer]     |
+---------------------------+--------------------------------+


scala>spark.stop

```

##### TF-IDF(특정 문자의 문서 내 중요도 수치 확인)

- TF-IDF(Term Frequency – Inverse Document Frequency) – 여러 문서 집합에서 특정 단어가 특정 문서 내에서 가지는 중요도를 수치화한 통계적 수치
- TF-IDF는 문서 내에서 단어의 출현 빈도를 나타내는 TF(단어 빈도)와 문서군 내에서 출현 빈도를 나타내는 IDF(문서 빈도,**빈도가 높을 수록 점수가 낮아짐**)의 조합으로 결정되며, 문서 내에서 출현 빈도가 높은 단어일수록 높은 점수를 부여하되 특정 문서가 아닌 모든 문서에서 동일한 현상이 나타나면 흔하게 사용되는 중요하지 않은 단어로 간주해서 가중치를 낮춰주는 방법을 사용한다.
- 스파크 MLlib에서 TF-IDF 알고리즘은 TF 처리에 해당하는 부분은 트랜스포머 클래스로, IDF에 해당하는 부분은 평가자 클래스로 제공하고 있습니다.

``` scala
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
import org.apache.spark.sql.SparkSession

object TfIDFSample {  
     def main(args: Array[String]) {    
     val spark = SparkSession .builder() .appName("TfIDFSample") .master("local[*]") .getOrCreate()   
     val df1 = spark.createDataFrame(Seq(      (0, "a a a b b c"),      
                                                            (0, "a b c"),     
                                                            (1, "a c a a d"))).toDF("label", "sentence")    
    val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")    // 각 문장을 단어로 분리
    val df2 = tokenizer.transform(df1)    
    val hashingTF = new HashingTF().setInputCol("words").setOutputCol("TF-Features").setNumFeatures(20)   
    val df3 = hashingTF.transform(df2)    df3.cache()   
    val idf = new IDF().setInputCol("TF-Features").setOutputCol("Final-Features")   
    val idfModel = idf.fit(df3)   
    val rescaledData = idfModel.transform(df3)    
    rescaledData.select("words", "TF-Features", "Final-Features").show(false)    
   spark.stop  
     }
}

```

##### StringIndexer

- StringIndexer – 문자열 컬럼에 대응하는 숫자형 컬럼을 생성하는 평가자. 
- StringIndexer는 문자열 레이블 컬럼에 적용하며 해당 컬럼의 모든 문자열에 노출 빈도에 따른 인덱스를 부여해서 숫자로 된 새로운 레이블 컬럼을 생성합니다.
- StringIndexer는 트랜스포머가 아닌 평가자로서 fit() 메서드를 이용해 stringIndexerModel을 생성하며 이 모델을 이용해 문자열 인코딩을 수행가능

```scala
import org.apache.spark.ml.feature.{IndexToString, StringIndexer}
import org.apache.spark.sql.SparkSession
object StringIndexerSample {  
def main(args: Array[String]) {   
   val spark = SparkSession .builder() .appName("StringIndexerSample") .master("local[*]") .getOrCreate() 
   val df1 = spark.createDataFrame(Seq(      (0, "red"),      (1, "blue"),      (2, "green"),      (3, "yellow"))).toDF("id", "color")   
  val strignIndexer = new StringIndexer().setInputCol("color") .setOutputCol("colorIndex") .fit(df1)    
  val df2 = strignIndexer.transform(df1)    
  df2.show(false)    
  val indexToString = new IndexToString() .setInputCol("colorIndex") .setOutputCol("originalColor") 
  val df3 = indexToString.transform(df2)   
  df3.show(false)    
   spark.stop 
 }
}

```

```scala
scala>    val df1 = spark.createDataFrame(Seq(      (0, "red"),      (1, "blue"),      (2, "green"),      (3, "yellow"))).toDF("id", "color")
df1: org.apache.spark.sql.DataFrame = [id: int, color: string]

//평가자에 해당
scala>   val strignIndexer = new StringIndexer().setInputCol("color") .setOutputCol("colorIndex") .fit(df1)
//fit을 호출해서 모델 생성
strignIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_a8880ed32ab1

scala>   val df2 = strignIndexer.transform(df1)
//transform 호출하는 것
df2: org.apache.spark.sql.DataFrame = [id: int, color: string ... 1 more field]

scala>   df2.show(false)
+---+------+----------+
|id |color |colorIndex|
+---+------+----------+
|0  |red   |1.0       |
|1  |blue  |2.0       |
|2  |green |3.0       |
|3  |yellow|0.0       |
+---+------+----------+

//index를 String으로 변환
scala>   val indexToString = new IndexToString() .setInputCol("colorIndex") .setOutputCol("originalColor")
indexToString: org.apache.spark.ml.feature.IndexToString = idxToStr_4b6a64a88aa6

scala>   val df3 = indexToString.transform(df2)
//String된 값을 확인하기 위함
df3: org.apache.spark.sql.DataFrame = [id: int, color: string ... 2 more fields]

scala>   df3.show(false)
+---+------+----------+-------------+
|id |color |colorIndex|originalColor|
+---+------+----------+-------------+
|0  |red   |1.0       |red          |
|1  |blue  |2.0       |blue         |
|2  |green |3.0       |green        |
|3  |yellow|0.0       |yellow       |
+---+------+----------+-------------+


scala>    spark.stop

```

##### 회귀로 매출 분석

```scala
hadoop fs -mkdir /data/sales
hadoop fs -put weather.csv  /data/sales/
hadoop fs -put sales.csv  /data/sales/

1단계 : 데이터 전처리
MLlib 입력 데이터 형으로 변환하기 위해 DataFrame으로 생성

shema 정의 - case class 정의


case class Weather( date: String,
                    day_of_week: String,
                    avg_temp: Double,
                    max_temp: Double,
                    min_temp: Double,
                    rainfall: Double,
                    daylight_hours: Double,
                    max_depth_snowfall: Double,
                    total_snowfall: Double,
                    solar_radiation: Double,
                    mean_wind_speed: Double,
                    max_wind_speed: Double,
                    max_instantaneous_wind_speed: Double,
                    avg_humidity: Double,
                    avg_cloud_cover: Double)
case class Sales(date: String, sales: Double)


import spark.implicits._
import org.apache.spark.mllib.regression.{LabeledPoint,LinearRegressionWithSGD}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.feature.StandardScaler
import org.apache.spark.mllib.evaluation.RegressionMetrics
import org.apache.spark.sql.functions.udf



// 기상 데이터를 읽어 DataFrame으로 변환한다
val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)
val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
      map(p => Weather(p(0),
      p(1),
      p(2).trim.toDouble,
      p(3).trim.toDouble,
      p(4).trim.toDouble,
      p(5).trim.toDouble,
      p(6).trim.toDouble,
      p(7).trim.toDouble,
      p(8).trim.toDouble,
      p(9).trim.toDouble,
      p(10).trim.toDouble,
      p(11).trim.toDouble,
      p(12).trim.toDouble,
      p(13).trim.toDouble,
      p(14).trim.toDouble
    )).toDF()

// 매출 데이터를 읽어 DataFrame으로 변환한다
val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()

//정의된 스키마 확인
println(weatherDF.printSchema)  
println(salesDF.printSchema)   
// 데이터의 전처리(날짜 기준으로 조인 후, 요일 컬럼값을 수치화하고, 요일컬럼제거후 , 수치화된 주말컬럼 추가)
val salesAndWeatherDF = salesDF.join(weatherDF, "date")
val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d  else 0d)
val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")

//매출에 영향을 주는 독립변수만 추출하여 새로운 데이터 프레임 생성
//매출에 영향을 주는 독립변수 평균기온, 일강수량, 휴일을 선택
val selectedDataDF = replacedSalesAndWeatherDF.select(
"sales","avg_temp","rainfall","weekend")

import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors

//데이터프레임을 회귀분석을 위한 Vector, LabeledPoint로 생성
 val labeledPoints = selectedDataDF.rdd.map(row => LabeledPoint(row.getDouble(0),
 Vectors.dense(row.getDouble(1),row.getDouble(2),row.getDouble(3))))
//데이터 특성을 표준화(평균 0, 분산1인 스케일러 사용)
import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(withMean = true, withStd = true).fit(labeledPointsRDD.map(x => x.features))
//

val scaler = new StandardScaler().fit(labeledPoints.map(x =>x.features))
val scaledLabledPointsRDD = labeledPoints.map(x => LabeledPoint(x.label, scaler.transform(x.features)))
//표준화 완료

// 선형회귀 모델을 작성한다
    val numIterations = 20
    scaledLabledPointsRDD.cache
    val linearRegressionModel = LinearRegressionWithSGD.train(scaledLabledPointsRDD, numIterations)
    println("weights :" + linearRegressionModel.weights)

// 알고리즘에 미지의 데이터를 적용해 예측한다
    val targetDataVector1 = Vectors.dense(15.0,15.4,1)
    val targetDataVector2 = Vectors.dense(20.0,0,0)
    val targetScaledDataVector1 = scaler.transform(targetDataVector1)
    val targetScaledDataVector2 = scaler.transform(targetDataVector2)
    val result1 = linearRegressionModel.predict(targetScaledDataVector1)
    val result2 = linearRegressionModel.predict(targetScaledDataVector2)
    println("avg_tmp=15.0,rainfall=15.4,weekend=true : sales = " + result1)
    println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)

// 입력 데이터를 분할하고 평가한다
    val splitScaledLabeledPointsRDD = scaledLabledPointsRDD.randomSplit(Array(0.6, 0.4), seed = 11L)
    val trainingScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(0).cache()
    val testScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(1)
    val linearRegressionModel2 = LinearRegressionWithSGD.train(trainingScaledLabeledPointsRDD, numIterations)
    val scoreAndLabels = testScaledLabeledPointsRDD.map { point =>
     val score = linearRegressionModel2.predict(point.features)
      (score, point.label)
    }
val metrics = new RegressionMetrics(scoreAndLabels)
    println("RMSE = "+ metrics.rootMeanSquaredError)
// 작성한 모델을 보존한다
linearRegressionModel.save(sc, "/output/mllib/model/") 

val model2 =LinearRegressionModel.load(sc, "/output/mllib/model/")
import org.apache.spark.mllib.regression.LinearRegressionModel
linearRegressionModel.toPMML("model.pmml")
//home에 저장되는데 위치를 모른다면 find / -name 'model.*'
```

```scala
//spark-shell --master local[*]
scala> case class Weather( date: String,
     |                     day_of_week: String,
     |                     avg_temp: Double,
     |                     max_temp: Double,
     |                     min_temp: Double,
     |                     rainfall: Double,
     |                     daylight_hours: Double,
     |                     max_depth_snowfall: Double,
     |                     total_snowfall: Double,
     |                     solar_radiation: Double,
     |                     mean_wind_speed: Double,
     |                     max_wind_speed: Double,
     |                     max_instantaneous_wind_speed: Double,
     |                     avg_humidity: Double,
     |                     avg_cloud_cover: Double)
defined class Weather

scala> case class Sales(date: String, sales: Double)
defined class Sales

scala>

scala> val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
weatherCSVRDD: org.apache.spark.rdd.RDD[String] = /data/sales/weather.csv MapPartitionsRDD[1] at textFile at <console>:32

scala> val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
headerOfWeatherCSVRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:34

scala> val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)
weatherCSVwithoutHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at subtract at <console>:35

scala> val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
     |       map(p => Weather(p(0),
     |       p(1),
     |       p(2).trim.toDouble,
     |       p(3).trim.toDouble,
     |       p(4).trim.toDouble,
     |       p(5).trim.toDouble,
     |       p(6).trim.toDouble,
     |       p(7).trim.toDouble,
     |       p(8).trim.toDouble,
     |       p(9).trim.toDouble,
     |       p(10).trim.toDouble,
     |       p(11).trim.toDouble,
     |       p(12).trim.toDouble,
     |       p(13).trim.toDouble,
     |       p(14).trim.toDouble
     |     )).toDF()
weatherDF: org.apache.spark.sql.DataFrame = [date: string, day_of_week: string ... 13 more fields]

scala> val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
salesCSVRDD: org.apache.spark.rdd.RDD[String] = /data/sales/sales.csv MapPartitionsRDD[10] at textFile at <console>:32

scala> val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
headerOfSalesCSVRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[11] at parallelize at <console>:34

scala> val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
salesCSVwithoutHeaderRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at subtract at <console>:35

scala> val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()
salesDF: org.apache.spark.sql.DataFrame = [date: string, sales: double]

scala> println(weatherDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- day_of_week: string (nullable = true)
 |-- avg_temp: double (nullable = false)
 |-- max_temp: double (nullable = false)
 |-- min_temp: double (nullable = false)
 |-- rainfall: double (nullable = false)
 |-- daylight_hours: double (nullable = false)
 |-- max_depth_snowfall: double (nullable = false)
 |-- total_snowfall: double (nullable = false)
 |-- solar_radiation: double (nullable = false)
 |-- mean_wind_speed: double (nullable = false)
 |-- max_wind_speed: double (nullable = false)
 |-- max_instantaneous_wind_speed: double (nullable = false)
 |-- avg_humidity: double (nullable = false)
 |-- avg_cloud_cover: double (nullable = false)

()

scala> println(salesDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- sales: double (nullable = false) 

()
scala>val salesAndWeatherDF = salesDF.join(weatherDF, "date")
salesAndWeatherDF: org.apache.spark.sql.DataFrame = [date: string, sales: double ... 14 more fields]

scala>val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d
     |                                        else 0d)
isWeekend: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))

scala>val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")
replacedSalesAndWeatherDF: org.apache.spark.sql.DataFrame = [date: string, sales: double ... 14 more fields]

scala>     val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d
     |                                        else 0d)
isWeekend: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))

scala>     val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")
replacedSalesAndWeatherDF: org.apache.spark.sql.DataFrame = [date: string, sales: double ... 14 more fields]

scala> val selectedDataDF = replacedSalesAndWeatherDF.select(
     | "sales","avg_temp","rainfall","weekend")
selectedDataDF: org.apache.spark.sql.DataFrame = [sales: double, avg_temp: double ... 2 more fields]



scala> val labeledPointsRDD = selectedDataDF.map(row => LabeledPoint(row.getDouble(0),
     |  Vectors.dense(row.getDouble(1),row.getDouble(2),row.getDouble(3))))
labeledPointsRDD: org.apache.spark.sql.Dataset[org.apache.spark.mllib.regression.LabeledPoint] = [label: double, features: vector]



//결과

scala>     println("weights :" + linearRegressionModel.weights)
weights :[199554.8706936455,30004.300595788496,158369.36233063164]

scala>  val targetDataVector1 = Vectors.dense(15.0,15.4,1)
targetDataVector1: org.apache.spark.mllib.linalg.Vector = [15.0,15.4,1.0]

scala>     val targetDataVector2 = Vectors.dense(20.0,0,0)
targetDataVector2: org.apache.spark.mllib.linalg.Vector = [20.0,0.0,0.0]

scala>     val targetScaledDataVector1 = scaler.transform(targetDataVector1)
targetScaledDataVector1: org.apache.spark.mllib.linalg.Vector = [1.6692119810227204,1.4033030983491395,2.208721944576236]

scala>     val targetScaledDataVector2 = scaler.transform(targetDataVector2)
targetScaledDataVector2: org.apache.spark.mllib.linalg.Vector = [2.225615974696961,0.0,0.0]

scala>     val result1 = linearRegressionModel.predict(targetScaledDataVector1)
result1: Double = 724998.3949513528

scala>     val result2 = linearRegressionModel.predict(targetScaledDataVector2)
result2: Double = 444132.5080443638

scala>     println("avg_tmp=15.0,rainfall=15.4,weekend=true : sales = " + result1)
avg_tmp=15.0,rainfall=15.4,weekend=true : sales = 724998.3949513528

scala>     println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)r)
<console>:1: error: ';' expected but ')' found.
    println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)r)
                                                                          ^

scala>     println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)
avg_tmp=20.0,rainfall=0,weekend=false : sales = 444132.5080443638


//평가하기
scala>     val splitScaledLabeledPointsRDD = scaledLabledPointsRDD.randomSplit(Array(0.6, 0.4), seed = 11L)
splitScaledLabeledPointsRDD: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[139] at randomSplit at <console>:40, MapPartitionsRDD[140] at randomSplit at <console>:40)

scala>     val trainingScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(0).cache()
trainingScaledLabeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[139] at randomSplit at <console>:40

scala>     val testScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(1)
testScaledLabeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[140] at randomSplit at <console>:40

scala>     val linearRegressionModel2 = LinearRegressionWithSGD.train(trainingScaledLabeledPointsRDD, numIterations)
warning: there was one deprecation warning; re-run with -deprecation for details
linearRegressionModel2: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 3

scala>     val scoreAndLabels = testScaledLabeledPointsRDD.map { point =>
     |      val score = linearRegressionModel2.predict(point.features)
     |       (score, point.label)
     |     }
scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[203] at map at <console>:42

scala>     val splitScaledLabeledPointsRDD = scaledLabledPointsRDD.randomSplit(Array(0.6, 0.4), seed = 11L)
splitScaledLabeledPointsRDD: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[204] at randomSplit at <console>:40, MapPartitionsRDD[205] at randomSplit at <console>:40)

scala>     val trainingScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(0).cache()
trainingScaledLabeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[204] at randomSplit at <console>:40

scala>     val testScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(1)
testScaledLabeledPointsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[205] at randomSplit at <console>:40

scala>     val linearRegressionModel2 = LinearRegressionWithSGD.train(trainingScaledLabeledPointsRDD, numIterations)
warning: there was one deprecation warning; re-run with -deprecation for details
[Stage 318:==================================>          [Stage 318:========================================>                                                            [Stage 342:===============================>                                                                     linearRegressionModel2: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 3

scala>     val scoreAndLabels = testScaledLabeledPointsRDD.map { point =>
     |      val score = linearRegressionModel2.predict(point.features)
     |       (score, point.label)
     |     }
scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[268] at map at <console>:42


```







#  참고로 알자

#### 파일 찾기

```cmd
[hadoop@master ~]$ hadoop fs -find / -name kmeans -print
```

### 스키마 확인 방법

```scala
print(schema)
```







